{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "6mQy2q--UHhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Hotel Reviews Sentiment Analysis\n",
        "#\n",
        "# ## Project Overview\n",
        "# This project focuses on performing sentiment analysis on hotel reviews to classify them as positive, negative, or neutral. We'll use various NLP techniques and machine learning models to analyze hotel review sentiments.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Import Required Libraries\n",
        "\n",
        "# %%\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import kagglehub\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Text Processing\n",
        "from textblob import TextBlob\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('vader_lexicon')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('punkt_tab') # Download punkt_tab as suggested by the error\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Disable LaTeX in matplotlib to avoid parsing errors\n",
        "plt.rcParams.update({\n",
        "    \"text.usetex\": False,\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"DejaVu Serif\"],\n",
        "})\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Load Dataset from Kaggle\n",
        "\n",
        "# %%\n",
        "# Download dataset from Kaggle\n",
        "print(\"Downloading Hotel Reviews dataset from Kaggle...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"jonathanoheix/sentiment-analysis-with-hotel-reviews\")\n",
        "    print(\"Dataset downloaded to:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Using fallback method...\")\n",
        "    path = \"./hotel-reviews\"\n",
        "\n",
        "# Explore the directory structure\n",
        "def explore_directory_structure(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 2 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for f in files[:10]:\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "explore_directory_structure(path)\n",
        "\n",
        "# %%\n",
        "# Find data files\n",
        "def find_data_files(directory):\n",
        "    data_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv') or file.endswith('.json') or file.endswith('.xlsx'):\n",
        "                data_files.append(os.path.join(root, file))\n",
        "    return data_files\n",
        "\n",
        "data_files = find_data_files(path)\n",
        "print(f\"Found data files: {data_files}\")\n",
        "\n",
        "# %%\n",
        "# Load the dataset\n",
        "def load_hotel_reviews_data(data_files):\n",
        "    for file_path in data_files:\n",
        "        try:\n",
        "            if file_path.endswith('.csv'):\n",
        "                df = pd.read_csv(file_path)\n",
        "                print(f\"Successfully loaded: {file_path}\")\n",
        "                print(f\"Dataset shape: {df.shape}\")\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    # If no file loaded successfully, create sample data\n",
        "    print(\"Creating sample hotel reviews data for demonstration...\")\n",
        "    return create_sample_hotel_reviews_data()\n",
        "\n",
        "def create_sample_hotel_reviews_data():\n",
        "    # Sample hotel reviews with different sentiments\n",
        "    sample_data = {\n",
        "        'Review': [\n",
        "            # Positive reviews\n",
        "            \"Absolutely loved our stay! The room was spacious, clean, and had a beautiful view. Staff was incredibly friendly and helpful.\",\n",
        "            \"Excellent hotel with amazing amenities. The breakfast was delicious and the pool area was fantastic. Will definitely return!\",\n",
        "            \"Perfect location, right in the city center. Room was modern and comfortable. Service exceeded our expectations.\",\n",
        "            \"Wonderful experience from check-in to check-out. The bed was extremely comfortable and the room was spotless.\",\n",
        "            \"Great value for money. The staff went above and beyond to make our anniversary special. Highly recommended!\",\n",
        "            \"Beautiful hotel with excellent facilities. The spa was relaxing and the restaurant served delicious food.\",\n",
        "            \"Outstanding service! The concierge helped us with all our tour bookings. Rooms are well-maintained and clean.\",\n",
        "            \"Best hotel I've stayed in! The attention to detail is remarkable. Can't wait to come back next year.\",\n",
        "\n",
        "            # Negative reviews\n",
        "            \"Terrible experience. Room was dirty, AC wasn't working, and staff was unhelpful. Would not recommend.\",\n",
        "            \"Very disappointed with our stay. The room smelled musty and the bathroom was not clean. Poor service overall.\",\n",
        "            \"Overpriced for what you get. Room was small, noisy, and the bed was uncomfortable. Avoid this hotel.\",\n",
        "            \"Awful customer service. They lost our reservation and were rude when we complained. Never staying here again.\",\n",
        "            \"Room was not as advertised. Stains on carpet, broken furniture, and slow wifi. Very disappointing.\",\n",
        "            \"The worst hotel experience ever. No hot water, loud construction noise, and unresponsive staff.\",\n",
        "            \"Dirty rooms and rude staff. The pool was closed and no one informed us beforehand. Complete waste of money.\",\n",
        "\n",
        "            # Neutral reviews\n",
        "            \"Average hotel with basic amenities. Nothing special but decent for a short stay. Location is convenient.\",\n",
        "            \"The room was okay, nothing exceptional. Service was acceptable. Would consider staying again if needed.\",\n",
        "            \"Standard hotel experience. Clean rooms but dated decor. Breakfast was average. Met basic expectations.\",\n",
        "            \"Fair accommodation for the price. Room was functional but lacked character. Staff was polite but not exceptional.\",\n",
        "            \"Mediocre experience overall. The hotel serves its purpose but doesn't stand out in any particular way.\"\n",
        "        ],\n",
        "        'Rating': [5, 5, 5, 5, 4, 5, 5, 5, 1, 2, 2, 1, 2, 1, 1, 3, 3, 3, 3, 3]\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "# Load the data\n",
        "df = load_hotel_reviews_data(data_files)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Data Exploration and Preprocessing\n",
        "\n",
        "# %%\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"\\nDataset Information:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# %%\n",
        "# Check column names and adjust accordingly\n",
        "print(\"Column names:\", df.columns.tolist())\n",
        "\n",
        "# Standardize column names\n",
        "if 'Review' in df.columns:\n",
        "    text_col = 'Review'\n",
        "elif 'review' in df.columns:\n",
        "    text_col = 'review'\n",
        "elif 'text' in df.columns:\n",
        "    text_col = 'text'\n",
        "elif 'description' in df.columns:\n",
        "    text_col = 'description'\n",
        "else:\n",
        "    # Try to find text column\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object' and len(df[col][0]) > 10:\n",
        "            text_col = col\n",
        "            break\n",
        "    text_col = df.columns[0]  # Fallback to first column\n",
        "\n",
        "if 'Rating' in df.columns:\n",
        "    rating_col = 'Rating'\n",
        "elif 'rating' in df.columns:\n",
        "    rating_col = 'rating'\n",
        "elif 'score' in df.columns:\n",
        "    rating_col = 'score'\n",
        "else:\n",
        "    # Try to find rating column\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            rating_col = col\n",
        "            break\n",
        "    rating_col = df.columns[1] if len(df.columns) > 1 else None\n",
        "\n",
        "print(f\"Using '{text_col}' as text column and '{rating_col}' as rating column\")\n",
        "\n",
        "# %%\n",
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values\n",
        "df_clean = df.dropna(subset=[text_col])\n",
        "if rating_col:\n",
        "    df_clean = df_clean.dropna(subset=[rating_col])\n",
        "\n",
        "print(f\"Dataset after cleaning: {df_clean.shape}\")\n",
        "\n",
        "# %%\n",
        "# Create sentiment labels based on ratings\n",
        "def create_sentiment_labels(df, rating_col):\n",
        "    if rating_col is None:\n",
        "        # If no rating column, we'll use text-based sentiment analysis\n",
        "        print(\"No rating column found. Using text-based sentiment analysis...\")\n",
        "        return None\n",
        "\n",
        "    df_with_sentiment = df.copy()\n",
        "\n",
        "    # Convert ratings to sentiment categories\n",
        "    def rating_to_sentiment(rating):\n",
        "        if rating >= 4:\n",
        "            return 'positive'\n",
        "        elif rating <= 2:\n",
        "            return 'negative'\n",
        "        else:\n",
        "            return 'neutral'\n",
        "\n",
        "    df_with_sentiment['sentiment'] = df_with_sentiment[rating_col].apply(rating_to_sentiment)\n",
        "    return df_with_sentiment\n",
        "\n",
        "df_with_sentiment = create_sentiment_labels(df_clean, rating_col)\n",
        "\n",
        "# If no rating column, use TextBlob for initial sentiment analysis\n",
        "if df_with_sentiment is None:\n",
        "    df_with_sentiment = df_clean.copy()\n",
        "    def text_to_sentiment(text):\n",
        "        analysis = TextBlob(str(text))\n",
        "        if analysis.sentiment.polarity > 0.1:\n",
        "            return 'positive'\n",
        "        elif analysis.sentiment.polarity < -0.1:\n",
        "            return 'negative'\n",
        "        else:\n",
        "            return 'neutral'\n",
        "\n",
        "    df_with_sentiment['sentiment'] = df_with_sentiment[text_col].apply(text_to_sentiment)\n",
        "\n",
        "print(\"Sentiment distribution:\")\n",
        "print(df_with_sentiment['sentiment'].value_counts())\n",
        "\n",
        "# %%\n",
        "# Visualize sentiment distribution\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "sentiment_counts = df_with_sentiment['sentiment'].value_counts()\n",
        "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
        "        colors=['lightgreen', 'lightcoral', 'lightblue'], startangle=90)\n",
        "plt.title('Sentiment Distribution')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.countplot(x='sentiment', data=df_with_sentiment, palette=['lightgreen', 'lightcoral', 'lightblue'])\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Rating distribution if available\n",
        "if rating_col:\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.hist(df_with_sentiment[rating_col], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.title('Rating Distribution')\n",
        "    plt.xlabel('Rating')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "# Text length analysis\n",
        "plt.subplot(2, 3, 4)\n",
        "df_with_sentiment['text_length'] = df_with_sentiment[text_col].apply(len)\n",
        "sns.boxplot(x='sentiment', y='text_length', data=df_with_sentiment,\n",
        "            palette=['lightgreen', 'lightcoral', 'lightblue'])\n",
        "plt.title('Text Length by Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Text Length')\n",
        "\n",
        "# Word count analysis\n",
        "plt.subplot(2, 3, 5)\n",
        "df_with_sentiment['word_count'] = df_with_sentiment[text_col].apply(lambda x: len(str(x).split()))\n",
        "sns.boxplot(x='sentiment', y='word_count', data=df_with_sentiment,\n",
        "            palette=['lightgreen', 'lightcoral', 'lightblue'])\n",
        "plt.title('Word Count by Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Word Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Text Preprocessing\n",
        "\n",
        "# %%\n",
        "# Text preprocessing functions\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'www\\S+', '', text)\n",
        "\n",
        "        # Remove HTML tags\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "        # Remove special characters and numbers (keep only letters and basic punctuation)\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\.\\,\\!\\?]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_lemmatize(self, text):\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def preprocess_text(self, text_series):\n",
        "        # Clean text\n",
        "        cleaned_text = text_series.apply(self.clean_text)\n",
        "\n",
        "        # Tokenize and lemmatize\n",
        "        processed_text = cleaned_text.apply(self.tokenize_and_lemmatize)\n",
        "\n",
        "        return processed_text\n",
        "\n",
        "# %%\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Preprocess text\n",
        "print(\"Preprocessing text...\")\n",
        "df_with_sentiment['processed_text'] = preprocessor.preprocess_text(df_with_sentiment[text_col])\n",
        "\n",
        "# Display before and after\n",
        "print(\"\\nOriginal vs Processed Text:\")\n",
        "sample_idx = 0\n",
        "if len(df_with_sentiment) > 0:\n",
        "    print(\"Original:\", df_with_sentiment[text_col].iloc[sample_idx][:100] + \"...\")\n",
        "    print(\"Processed:\", df_with_sentiment['processed_text'].iloc[sample_idx][:100] + \"...\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Feature Engineering\n",
        "\n",
        "# %%\n",
        "# Advanced Feature Engineering\n",
        "def extract_advanced_features(df):\n",
        "    df_features = df.copy()\n",
        "\n",
        "    # Basic text features\n",
        "    df_features['char_count'] = df_features[text_col].apply(len)\n",
        "    df_features['word_count'] = df_features[text_col].apply(lambda x: len(str(x).split()))\n",
        "    df_features['sentence_count'] = df_features[text_col].apply(lambda x: len(sent_tokenize(str(x))))\n",
        "    df_features['avg_word_length'] = df_features['char_count'] / df_features['word_count']\n",
        "    df_features['avg_sentence_length'] = df_features['word_count'] / df_features['sentence_count']\n",
        "\n",
        "    # Sentiment scores using TextBlob\n",
        "    df_features['textblob_polarity'] = df_features[text_col].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
        "    df_features['textblob_subjectivity'] = df_features[text_col].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
        "\n",
        "    # VADER sentiment analysis\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    df_features['vader_compound'] = df_features[text_col].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
        "    df_features['vader_positive'] = df_features[text_col].apply(lambda x: sia.polarity_scores(str(x))['pos'])\n",
        "    df_features['vader_negative'] = df_features[text_col].apply(lambda x: sia.polarity_scores(str(x))['neg'])\n",
        "    df_features['vader_neutral'] = df_features[text_col].apply(lambda x: sia.polarity_scores(str(x))['neu'])\n",
        "\n",
        "    # Emotional indicators\n",
        "    df_features['exclamation_count'] = df_features[text_col].apply(lambda x: str(x).count('!'))\n",
        "    df_features['question_count'] = df_features[text_col].apply(lambda x: str(x).count('?'))\n",
        "    df_features['uppercase_ratio'] = df_features[text_col].apply(\n",
        "        lambda x: sum(1 for c in str(x) if c.isupper()) / len(str(x)) if len(str(x)) > 0 else 0\n",
        "    )\n",
        "\n",
        "    # Hotel-specific features\n",
        "    hotel_keywords = {\n",
        "        'room': ['room', 'bed', 'bathroom', 'view', 'clean', 'dirty', 'spacious', 'small'],\n",
        "        'service': ['staff', 'service', 'friendly', 'rude', 'helpful', 'concierge'],\n",
        "        'facilities': ['pool', 'gym', 'spa', 'restaurant', 'breakfast', 'wifi', 'parking'],\n",
        "        'location': ['location', 'central', 'convenient', 'access', 'transport']\n",
        "    }\n",
        "\n",
        "    for category, keywords in hotel_keywords.items():\n",
        "        df_features[f'{category}_mentions'] = df_features[text_col].apply(\n",
        "            lambda x: sum(1 for word in keywords if word in str(x).lower())\n",
        "        )\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# %%\n",
        "# Extract features\n",
        "print(\"Extracting advanced features...\")\n",
        "df_features = extract_advanced_features(df_with_sentiment)\n",
        "\n",
        "# Display feature correlations with sentiment\n",
        "label_encoder = LabelEncoder()\n",
        "df_features['sentiment_encoded'] = label_encoder.fit_transform(df_features['sentiment'])\n",
        "\n",
        "numeric_features = df_features.select_dtypes(include=[np.number]).columns\n",
        "correlation_with_target = df_features[numeric_features].corr()['sentiment_encoded'].sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nFeature correlations with sentiment:\")\n",
        "print(correlation_with_target.head(10))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Data Visualization\n",
        "\n",
        "# %%\n",
        "# Word Clouds for different sentiments\n",
        "def generate_wordclouds(df, text_col, sentiment_col):\n",
        "    sentiments = df[sentiment_col].unique()\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i, sentiment in enumerate(sentiments):\n",
        "        plt.subplot(1, len(sentiments), i+1)\n",
        "\n",
        "        # Combine all text for this sentiment\n",
        "        text = ' '.join(df[df[sentiment_col] == sentiment][text_col].astype(str))\n",
        "\n",
        "        # Generate word cloud\n",
        "        wordcloud = WordCloud(\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white',\n",
        "            max_words=100,\n",
        "            colormap='viridis' if sentiment == 'positive' else 'Reds' if sentiment == 'negative' else 'Blues'\n",
        "        ).generate(text)\n",
        "\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'{sentiment.capitalize()} Reviews Word Cloud')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate word clouds\n",
        "generate_wordclouds(df_with_sentiment, 'processed_text', 'sentiment')\n",
        "\n",
        "# %%\n",
        "# Sentiment score distributions\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "sentiment_features = ['textblob_polarity', 'textblob_subjectivity', 'vader_compound']\n",
        "\n",
        "for i, feature in enumerate(sentiment_features):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    for sentiment in df_features['sentiment'].unique():\n",
        "        data = df_features[df_features['sentiment'] == sentiment][feature]\n",
        "        plt.hist(data, alpha=0.6, label=sentiment, bins=20)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "\n",
        "# Feature importance visualization\n",
        "plt.subplot(2, 3, 4)\n",
        "top_features = correlation_with_target.head(10)\n",
        "sns.barplot(x=top_features.values, y=top_features.index, palette='viridis')\n",
        "plt.title('Top 10 Features Correlated with Sentiment')\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Train-Test Split\n",
        "\n",
        "# %%\n",
        "# Prepare features and target\n",
        "X = df_features['processed_text']\n",
        "y = df_features['sentiment']\n",
        "\n",
        "# 80-20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Training class distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"Test class distribution:\\n{y_test.value_counts()}\")\n",
        "\n",
        "# Encode labels for neural network\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "y_train_categorical = to_categorical(y_train_encoded)\n",
        "y_test_categorical = to_categorical(y_test_encoded)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Traditional Machine Learning Models\n",
        "\n",
        "# %%\n",
        "# TF-IDF Vectorization\n",
        "print(\"Creating TF-IDF features...\")\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.8\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF Training shape: {X_train_tfidf.shape}\")\n",
        "print(f\"TF-IDF Test shape: {X_test_tfidf.shape}\")\n",
        "\n",
        "# %%\n",
        "# Define models to evaluate\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Multinomial Naive Bayes': MultinomialNB(),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# %%\n",
        "# Train and evaluate models\n",
        "def evaluate_models(models, X_train, X_test, y_train, y_test):\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "        train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
        "        test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'train_accuracy': train_accuracy,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'train_f1': train_f1,\n",
        "            'test_f1': test_f1,\n",
        "            'predictions': y_test_pred\n",
        "        }\n",
        "\n",
        "        print(f\"{name} Results:\")\n",
        "        print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"  Training F1: {train_f1:.4f}\")\n",
        "        print(f\"  Test F1: {test_f1:.4f}\")\n",
        "\n",
        "        # Classification report\n",
        "        print(f\"\\n  Classification Report for {name}:\")\n",
        "        print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "    return results\n",
        "\n",
        "# %%\n",
        "# Evaluate traditional models\n",
        "print(\"EVALUATING TRADITIONAL MACHINE LEARNING MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "ml_results = evaluate_models(models, X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Deep Learning Model (LSTM)\n",
        "\n",
        "# %%\n",
        "# Prepare data for LSTM\n",
        "print(\"Preparing data for LSTM...\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "max_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "print(f\"Padded sequences shape - Train: {X_train_pad.shape}, Test: {X_test_pad.shape}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
        "\n",
        "# %%\n",
        "# Build LSTM model\n",
        "def create_lstm_model(vocab_size=5000, embedding_dim=100, max_length=100, num_classes=3):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        SpatialDropout1D(0.2),\n",
        "        Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)),\n",
        "        Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# %%\n",
        "# Create and display model\n",
        "lstm_model = create_lstm_model(vocab_size=5000, embedding_dim=100, max_length=max_length, num_classes=3)\n",
        "lstm_model.summary()\n",
        "\n",
        "# %%\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7),\n",
        "    ModelCheckpoint('best_lstm_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "]\n",
        "\n",
        "# %%\n",
        "# Train LSTM model\n",
        "print(\"Training LSTM model...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train_pad, y_train_categorical,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_pad, y_test_categorical),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Evaluate LSTM model\n",
        "lstm_train_pred_proba = lstm_model.predict(X_train_pad)\n",
        "lstm_test_pred_proba = lstm_model.predict(X_test_pad)\n",
        "\n",
        "lstm_train_pred = np.argmax(lstm_train_pred_proba, axis=1)\n",
        "lstm_test_pred = np.argmax(lstm_test_pred_proba, axis=1)\n",
        "\n",
        "lstm_train_accuracy = accuracy_score(y_train_encoded, lstm_train_pred)\n",
        "lstm_test_accuracy = accuracy_score(y_test_encoded, lstm_test_pred)\n",
        "lstm_train_f1 = f1_score(y_train_encoded, lstm_train_pred, average='weighted')\n",
        "lstm_test_f1 = f1_score(y_test_encoded, lstm_test_pred, average='weighted')\n",
        "\n",
        "print(\"LSTM Model Results:\")\n",
        "print(f\"Training Accuracy: {lstm_train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {lstm_test_accuracy:.4f}\")\n",
        "print(f\"Training F1: {lstm_train_f1:.4f}\")\n",
        "print(f\"Test F1: {lstm_test_f1:.4f}\")\n",
        "\n",
        "print(\"\\nLSTM Classification Report:\")\n",
        "print(classification_report(y_test_encoded, lstm_test_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Store LSTM results\n",
        "ml_results['LSTM'] = {\n",
        "    'model': lstm_model,\n",
        "    'train_accuracy': lstm_train_accuracy,\n",
        "    'test_accuracy': lstm_test_accuracy,\n",
        "    'train_f1': lstm_train_f1,\n",
        "    'test_f1': lstm_test_f1,\n",
        "    'predictions': label_encoder.inverse_transform(lstm_test_pred)\n",
        "}\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Model Comparison and Visualization\n",
        "\n",
        "# %%\n",
        "# Compare all models\n",
        "models_comparison = pd.DataFrame({\n",
        "    'Model': list(ml_results.keys()),\n",
        "    'Train_Accuracy': [results['train_accuracy'] for results in ml_results.values()],\n",
        "    'Test_Accuracy': [results['test_accuracy'] for results in ml_results.values()],\n",
        "    'Train_F1': [results['train_f1'] for results in ml_results.values()],\n",
        "    'Test_F1': [results['test_f1'] for results in ml_results.values()],\n",
        "    'Overfitting_Score': [results['train_accuracy'] - results['test_accuracy'] for results in ml_results.values()]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(models_comparison)\n",
        "\n",
        "# %%\n",
        "# Visualize model comparison\n",
        "plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.barplot(x='Model', y='Train_Accuracy', data=models_comparison, palette='viridis', alpha=0.7, label='Train')\n",
        "sns.barplot(x='Model', y='Test_Accuracy', data=models_comparison, palette='viridis', alpha=0.5, label='Test')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# F1 Score comparison\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.barplot(x='Model', y='Train_F1', data=models_comparison, palette='coolwarm', alpha=0.7, label='Train')\n",
        "sns.barplot(x='Model', y='Test_F1', data=models_comparison, palette='coolwarm', alpha=0.5, label='Test')\n",
        "plt.title('Model F1 Score Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Overfitting analysis\n",
        "plt.subplot(2, 3, 3)\n",
        "sns.barplot(x='Model', y='Overfitting_Score', data=models_comparison, palette='RdYlBu_r')\n",
        "plt.title('Overfitting Analysis (Train Acc - Test Acc)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "\n",
        "# Training history for LSTM\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('LSTM Training History - Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Training History - Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion matrix for best model\n",
        "plt.subplot(2, 3, 6)\n",
        "best_model_name = models_comparison.loc[models_comparison['Test_F1'].idxmax(), 'Model']\n",
        "best_predictions = ml_results[best_model_name]['predictions']\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Interactive Sentiment Analysis\n",
        "\n",
        "# %%\n",
        "def predict_sentiment(text, model_type='best'):\n",
        "    \"\"\"\n",
        "    Predict sentiment of a given hotel review text\n",
        "    \"\"\"\n",
        "    if model_type == 'best':\n",
        "        model_name = best_model_name\n",
        "        model = ml_results[best_model_name]['model']\n",
        "    else:\n",
        "        model_name = model_type\n",
        "        model = ml_results[model_type]['model']\n",
        "\n",
        "    # Preprocess text\n",
        "    processed_text = preprocessor.preprocess_text(pd.Series([text])).iloc[0]\n",
        "\n",
        "    if model_name == 'LSTM':\n",
        "        # LSTM prediction\n",
        "        sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "        padded = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "        prediction_proba = model.predict(padded)[0]\n",
        "        predicted_class = np.argmax(prediction_proba)\n",
        "        sentiment = label_encoder.inverse_transform([predicted_class])[0]\n",
        "        confidence = np.max(prediction_proba)\n",
        "\n",
        "        # Get probabilities for all classes\n",
        "        probabilities = {label_encoder.inverse_transform([i])[0]: float(prediction_proba[i])\n",
        "                        for i in range(len(prediction_proba))}\n",
        "    else:\n",
        "        # Traditional ML prediction\n",
        "        features = tfidf.transform([processed_text])\n",
        "        prediction_proba = model.predict_proba(features)[0]\n",
        "        predicted_class = np.argmax(prediction_proba)\n",
        "        sentiment = model.classes_[predicted_class]\n",
        "        confidence = np.max(prediction_proba)\n",
        "\n",
        "        # Get probabilities for all classes\n",
        "        probabilities = {model.classes_[i]: float(prediction_proba[i])\n",
        "                        for i in range(len(prediction_proba))}\n",
        "\n",
        "    # Additional sentiment analysis\n",
        "    textblob_sentiment = TextBlob(text).sentiment\n",
        "    vader_sentiment = SentimentIntensityAnalyzer().polarity_scores(text)\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'sentiment': sentiment,\n",
        "        'confidence': confidence,\n",
        "        'probabilities': probabilities,\n",
        "        'model': model_name,\n",
        "        'textblob_polarity': textblob_sentiment.polarity,\n",
        "        'textblob_subjectivity': textblob_sentiment.subjectivity,\n",
        "        'vader_scores': vader_sentiment\n",
        "    }\n",
        "\n",
        "# %%\n",
        "def interactive_sentiment_analyzer():\n",
        "    \"\"\"\n",
        "    Interactive function for users to analyze hotel review sentiments\n",
        "    \"\"\"\n",
        "    print(\"HOTEL REVIEW SENTIMENT ANALYZER\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Available models:\", list(ml_results.keys()))\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        text = input(\"Enter a hotel review to analyze (or 'quit' to exit): \").strip()\n",
        "\n",
        "        if text.lower() == 'quit':\n",
        "            print(\"Thank you for using the Hotel Review Sentiment Analyzer!\")\n",
        "            break\n",
        "\n",
        "        if not text:\n",
        "            print(\"Please enter some text.\")\n",
        "            continue\n",
        "\n",
        "        # Get prediction from best model\n",
        "        result = predict_sentiment(text)\n",
        "\n",
        "        print(f\"\\nSentiment Analysis Results:\")\n",
        "        print(f\"Review: '{result['text'][:100]}...'\")\n",
        "        print(f\"Model: {result['model']}\")\n",
        "        print(f\"Predicted Sentiment: {result['sentiment'].upper()}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "\n",
        "        print(f\"\\nDetailed Probabilities:\")\n",
        "        for sentiment, prob in result['probabilities'].items():\n",
        "            print(f\"  {sentiment}: {prob:.4f}\")\n",
        "\n",
        "        print(f\"\\nAdditional Analysis:\")\n",
        "        print(f\"  TextBlob Polarity: {result['textblob_polarity']:.4f}\")\n",
        "        print(f\"  TextBlob Subjectivity: {result['textblob_subjectivity']:.4f}\")\n",
        "        print(f\"  VADER Scores: {result['vader_scores']}\")"
      ],
      "metadata": {
        "id": "Rmj2ByfOYMwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "feAqZIRngH92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Cryptocurrency Price Prediction\n",
        "# ## Complete Forecasting Pipeline\n",
        "#\n",
        "# This notebook implements a comprehensive cryptocurrency price forecasting system using machine learning and deep learning approaches.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Import Required Libraries\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Load Dataset from Kaggle\n",
        "\n",
        "# %%\n",
        "# Download dataset from Kaggle\n",
        "print(\"Downloading Cryptocurrency dataset from Kaggle...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"taniaj/cryptocurrency-price-forecasting\")\n",
        "    print(\"Dataset downloaded to:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Using fallback method...\")\n",
        "    path = \"./crypto-data\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# %%\n",
        "# Explore the directory structure\n",
        "def explore_directory_structure(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 2 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for f in files[:10]:\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "explore_directory_structure(path)\n",
        "\n",
        "# %%\n",
        "# Find data files\n",
        "def find_data_files(directory):\n",
        "    data_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv') or file.endswith('.json') or file.endswith('.xlsx'):\n",
        "                data_files.append(os.path.join(root, file))\n",
        "    return data_files\n",
        "\n",
        "data_files = find_data_files(path)\n",
        "print(f\"Found data files: {data_files}\")\n",
        "\n",
        "# %%\n",
        "# Function to create sample data if Kaggle dataset is not available\n",
        "def create_sample_crypto_data():\n",
        "    dates = pd.date_range(start='2020-01-01', end='2024-01-01', freq='D')\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Simulate BTC price with some realistic patterns\n",
        "    price = 7000\n",
        "    prices = []\n",
        "    for i in range(len(dates)):\n",
        "        # Random walk with some volatility\n",
        "        change = np.random.normal(0, 0.02)\n",
        "        # Add some seasonality\n",
        "        seasonality = 0.1 * np.sin(i / 365 * 2 * np.pi)\n",
        "        price = price * (1 + change + seasonality * 0.1)\n",
        "        prices.append(price)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Open': prices,\n",
        "        'High': [p * (1 + np.random.uniform(0, 0.05)) for p in prices],\n",
        "        'Low': [p * (1 - np.random.uniform(0, 0.05)) for p in prices],\n",
        "        'Close': prices,\n",
        "        'Volume': np.random.uniform(1e9, 5e9, len(dates)),\n",
        "        'Market_Cap': np.random.uniform(100e9, 500e9, len(dates))\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the dataset\n",
        "def load_crypto_data(data_files):\n",
        "    for file_path in data_files:\n",
        "        try:\n",
        "            if file_path.endswith('.csv'):\n",
        "                df = pd.read_csv(file_path)\n",
        "                print(f\"Successfully loaded: {file_path}\")\n",
        "                print(f\"Dataset shape: {df.shape}\")\n",
        "                print(f\"Columns: {df.columns.tolist()}\")\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    # If no file loaded successfully, create sample data\n",
        "    print(\"Creating sample cryptocurrency data for demonstration...\")\n",
        "    return create_sample_crypto_data()\n",
        "\n",
        "# Load the data\n",
        "df = load_crypto_data(data_files)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Data Exploration and Visualization\n",
        "\n",
        "# %%\n",
        "# Basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# %%\n",
        "# Price visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(df['Date'] if 'Date' in df.columns else df.index, df['Close'])\n",
        "plt.title('Cryptocurrency Closing Price')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(df['Close'], bins=50, alpha=0.7, edgecolor='black')\n",
        "plt.title('Price Distribution')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "daily_returns = df['Close'].pct_change().dropna()\n",
        "plt.hist(daily_returns, bins=100, alpha=0.7, edgecolor='black')\n",
        "plt.title('Daily Returns Distribution')\n",
        "plt.xlabel('Daily Return')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "volume = df['Volume'] if 'Volume' in df.columns else df['Close'] * 1000\n",
        "plt.plot(df['Date'] if 'Date' in df.columns else df.index, volume)\n",
        "plt.title('Trading Volume')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Feature Engineering\n",
        "\n",
        "# %%\n",
        "import pandas_ta as ta\n",
        "\n",
        "def create_technical_indicators(df):\n",
        "    \"\"\"Create technical indicators for the cryptocurrency data\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Ensure we have a datetime index\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df.set_index('Date', inplace=True)\n",
        "\n",
        "    # Price-based features\n",
        "    df['Price_Range'] = df['High'] - df['Low']\n",
        "    df['Price_Gap'] = df['Open'] - df['Close'].shift(1)\n",
        "\n",
        "    # Moving averages\n",
        "    df['SMA_7'] = df['Close'].rolling(window=7).mean()\n",
        "    df['SMA_21'] = df['Close'].rolling(window=21).mean()\n",
        "    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "\n",
        "    # Exponential moving averages\n",
        "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
        "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
        "\n",
        "    # RSI\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # MACD\n",
        "    exp1 = df['Close'].ewm(span=12).mean()\n",
        "    exp2 = df['Close'].ewm(span=26).mean()\n",
        "    df['MACD'] = exp1 - exp2\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "\n",
        "    # Bollinger Bands\n",
        "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "    bb_std = df['Close'].rolling(window=20).std()\n",
        "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
        "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
        "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
        "\n",
        "    # Volume indicators\n",
        "    if 'Volume' in df.columns:\n",
        "        df['Volume_SMA'] = df['Volume'].rolling(window=20).mean()\n",
        "        df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
        "\n",
        "    # Price momentum\n",
        "    df['Momentum_1D'] = df['Close'].pct_change(periods=1)\n",
        "    df['Momentum_7D'] = df['Close'].pct_change(periods=7)\n",
        "    df['Momentum_30D'] = df['Close'].pct_change(periods=30)\n",
        "\n",
        "    # Volatility\n",
        "    df['Volatility_7D'] = df['Close'].pct_change().rolling(window=7).std()\n",
        "    df['Volatility_30D'] = df['Close'].pct_change().rolling(window=30).std()\n",
        "\n",
        "    # Support and resistance levels\n",
        "    df['Resistance'] = df['High'].rolling(window=20).max()\n",
        "    df['Support'] = df['Low'].rolling(window=20).min()\n",
        "\n",
        "    # Drop NaN values created by rolling windows\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "df_features = create_technical_indicators(df)\n",
        "print(f\"Dataset shape after feature engineering: {df_features.shape}\")\n",
        "print(f\"New features created: {[col for col in df_features.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume', 'Market_Cap']]}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Data Preprocessing\n",
        "\n",
        "# %%\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_data_for_training(df, target_col='Close', lookback_days=60, forecast_days=7):\n",
        "    \"\"\"Prepare data for time series forecasting\"\"\"\n",
        "\n",
        "    # Select features for training\n",
        "    feature_columns = [col for col in df.columns if col not in ['Open', 'High', 'Low', 'Volume', 'Market_Cap']]\n",
        "\n",
        "    # Handle missing values in feature columns\n",
        "    for col in feature_columns:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "    # Create sequences for LSTM\n",
        "    X, y = [], []\n",
        "    for i in range(lookback_days, len(scaled_data) - forecast_days):\n",
        "        X.append(scaled_data[i-lookback_days:i])\n",
        "        # Predict the next 'forecast_days' closing prices\n",
        "        y.append(scaled_data[i:i+forecast_days, feature_columns.index(target_col)])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Split the data\n",
        "    split_idx = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler, feature_columns\n",
        "\n",
        "# Prepare the data\n",
        "X_train, X_test, y_train, y_test, scaler, feature_columns = prepare_data_for_training(df_features)\n",
        "\n",
        "print(f\"Training data shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
        "print(f\"Testing data shape: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
        "print(f\"Feature columns: {feature_columns}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Model Building - LSTM Neural Network\n",
        "\n",
        "# %%\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def build_lstm_model(input_shape, output_days=7):\n",
        "    \"\"\"Build and compile LSTM model\"\"\"\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(128, return_sequences=True, input_shape=input_shape)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Bidirectional(LSTM(32, return_sequences=False)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "\n",
        "        Dense(output_days)  # Predict multiple days ahead\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae', 'mape']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "model = build_lstm_model((X_train.shape[1], X_train.shape[2]), output_days=y_train.shape[1])\n",
        "model.summary()\n",
        "\n",
        "# %%\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "print(\"Training LSTM model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Model Evaluation\n",
        "\n",
        "# %%\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Create inverse scaling array for the target variable\n",
        "target_idx = feature_columns.index('Close')\n",
        "dummy_array = np.zeros((len(y_pred), len(feature_columns)))\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred_inverse = []\n",
        "y_test_inverse = []\n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "    # For predictions\n",
        "    dummy_array_pred = np.zeros((y_pred.shape[1], len(feature_columns)))\n",
        "    dummy_array_pred[:, target_idx] = y_pred[i]\n",
        "    y_pred_inverse.append(scaler.inverse_transform(dummy_array_pred)[:, target_idx])\n",
        "\n",
        "    # For actual values\n",
        "    dummy_array_test = np.zeros((y_test.shape[1], len(feature_columns)))\n",
        "    dummy_array_test[:, target_idx] = y_test[i]\n",
        "    y_test_inverse.append(scaler.inverse_transform(dummy_array_test)[:, target_idx])\n",
        "\n",
        "y_pred_inverse = np.array(y_pred_inverse)\n",
        "y_test_inverse = np.array(y_test_inverse)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"Calculate evaluation metrics\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R2_Score': r2\n",
        "    }\n",
        "\n",
        "# Calculate metrics for each forecast day\n",
        "metrics_per_day = []\n",
        "for day in range(y_test_inverse.shape[1]):\n",
        "    metrics = calculate_metrics(y_test_inverse[:, day], y_pred_inverse[:, day])\n",
        "    metrics['Day'] = day + 1\n",
        "    metrics_per_day.append(metrics)\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_per_day)\n",
        "print(\"\\nModel Performance by Forecast Day:\")\n",
        "print(metrics_df.round(4))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Visualization of Predictions\n",
        "\n",
        "# %%\n",
        "# Plot predictions vs actual for different forecast horizons\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "forecast_days_to_plot = [0, 2, 4, 6]  # Plot days 1, 3, 5, 7\n",
        "\n",
        "for i, day_idx in enumerate(forecast_days_to_plot):\n",
        "    axes[i].scatter(y_test_inverse[:, day_idx], y_pred_inverse[:, day_idx], alpha=0.6)\n",
        "    axes[i].plot([y_test_inverse[:, day_idx].min(), y_test_inverse[:, day_idx].max()],\n",
        "                [y_test_inverse[:, day_idx].min(), y_test_inverse[:, day_idx].max()], 'r--', lw=2)\n",
        "    axes[i].set_xlabel('Actual Price')\n",
        "    axes[i].set_ylabel('Predicted Price')\n",
        "    axes[i].set_title(f'Day {day_idx + 1} Forecast vs Actual')\n",
        "\n",
        "    # Add R² to plot\n",
        "    r2 = r2_score(y_test_inverse[:, day_idx], y_pred_inverse[:, day_idx])\n",
        "    axes[i].text(0.05, 0.95, f'R² = {r2:.3f}', transform=axes[i].transAxes,\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Time series plot of predictions\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Plot last 100 test samples for visualization\n",
        "sample_range = slice(-100, None)\n",
        "\n",
        "plt.plot(y_test_inverse[sample_range, 0], label='Actual Day 1', alpha=0.7)\n",
        "plt.plot(y_pred_inverse[sample_range, 0], label='Predicted Day 1', alpha=0.7)\n",
        "plt.plot(y_test_inverse[sample_range, 3], label='Actual Day 4', alpha=0.7)\n",
        "plt.plot(y_pred_inverse[sample_range, 3], label='Predicted Day 4', alpha=0.7)\n",
        "plt.plot(y_test_inverse[sample_range, 6], label='Actual Day 7', alpha=0.7)\n",
        "plt.plot(y_pred_inverse[sample_range, 6], label='Predicted Day 7', alpha=0.7)\n",
        "\n",
        "plt.title('Cryptocurrency Price Predictions vs Actual Values')\n",
        "plt.xlabel('Test Sample Index')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Future Price Prediction\n",
        "\n",
        "# %%\n",
        "def predict_future_prices(model, last_sequence, scaler, feature_columns, days_ahead=30):\n",
        "    \"\"\"Predict future prices using the trained model\"\"\"\n",
        "    future_predictions = []\n",
        "    current_sequence = last_sequence.copy()\n",
        "\n",
        "    target_idx = feature_columns.index('Close')\n",
        "\n",
        "    for _ in range(days_ahead):\n",
        "        # Predict next day\n",
        "        next_pred = model.predict(current_sequence.reshape(1, *current_sequence.shape), verbose=0)\n",
        "\n",
        "        # Get the first day prediction\n",
        "        next_day_pred = next_pred[0][0]\n",
        "        future_predictions.append(next_day_pred)\n",
        "\n",
        "        # Create new sequence by shifting and adding prediction\n",
        "        new_row = current_sequence[-1].copy()\n",
        "        new_row[target_idx] = next_day_pred\n",
        "\n",
        "        # Update sequence (remove first, add new prediction)\n",
        "        current_sequence = np.vstack([current_sequence[1:], new_row])\n",
        "\n",
        "    # Convert predictions back to original scale\n",
        "    dummy_array = np.zeros((len(future_predictions), len(feature_columns)))\n",
        "    dummy_array[:, target_idx] = future_predictions\n",
        "    future_prices = scaler.inverse_transform(dummy_array)[:, target_idx]\n",
        "\n",
        "    return future_prices\n",
        "\n",
        "# Get the last sequence from the data\n",
        "last_sequence = X_test[-1]\n",
        "\n",
        "# Predict next 30 days\n",
        "future_prices = predict_future_prices(model, last_sequence, scaler, feature_columns, days_ahead=30)\n",
        "\n",
        "# Create future dates\n",
        "last_date = df_features.index[-1]\n",
        "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30, freq='D')\n",
        "\n",
        "# Plot historical data and future predictions\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Plot last 180 days of historical data\n",
        "historical_dates = df_features.index[-180:]\n",
        "historical_prices = df_features['Close'][-180:]\n",
        "\n",
        "plt.plot(historical_dates, historical_prices, label='Historical Prices', linewidth=2)\n",
        "plt.plot(future_dates, future_prices, label='Predicted Future Prices', linewidth=2, color='red')\n",
        "plt.fill_between(future_dates, future_prices * 0.95, future_prices * 1.05, alpha=0.2, color='red', label='Prediction Range')\n",
        "\n",
        "plt.title('Cryptocurrency Price Forecast - Next 30 Days')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFuture Price Predictions:\")\n",
        "future_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Predicted_Price': future_prices\n",
        "})\n",
        "print(future_df.head(10))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Model Saving and Deployment\n",
        "\n",
        "# %%\n",
        "# Save the trained model\n",
        "model.save('cryptocurrency_price_predictor.h5')\n",
        "print(\"Model saved as 'cryptocurrency_price_predictor.h5'\")\n",
        "\n",
        "# Save the scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, 'price_scaler.pkl')\n",
        "print(\"Scaler saved as 'price_scaler.pkl'\")\n",
        "\n",
        "# Save feature columns\n",
        "import json\n",
        "with open('feature_columns.json', 'w') as f:\n",
        "    json.dump(feature_columns, f)\n",
        "print(\"Feature columns saved as 'feature_columns.json'\")\n",
        "\n",
        "# Create a summary report\n",
        "report = {\n",
        "    'model_performance': metrics_df.to_dict(),\n",
        "    'training_samples': len(X_train),\n",
        "    'testing_samples': len(X_test),\n",
        "    'lookback_days': X_train.shape[1],\n",
        "    'forecast_days': y_train.shape[1],\n",
        "    'best_val_loss': min(history.history['val_loss']),\n",
        "    'final_features': feature_columns\n",
        "}\n",
        "\n",
        "with open('model_report.json', 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "print(\"Model report saved as 'model_report.json'\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "#\n",
        "# This notebook demonstrates a complete pipeline for cryptocurrency price prediction using:\n",
        "# - Comprehensive feature engineering with technical indicators\n",
        "# - LSTM neural networks for time series forecasting\n",
        "# - Multi-day ahead predictions\n",
        "# - Model evaluation and future price forecasting\n",
        "#\n",
        "# The model can be further improved by:\n",
        "# - Incorporating external factors (news sentiment, market data)\n",
        "# - Using ensemble methods\n",
        "# - Hyperparameter tuning\n",
        "# - Adding more cryptocurrency pairs"
      ],
      "metadata": {
        "id": "l0dVANPigJxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "1I5WHhNZjHHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Emotion Detection from Facial Expressions\n",
        "# ## Complete Deep Learning Pipeline\n",
        "#\n",
        "# This notebook implements a comprehensive emotion detection system using the FER (Facial Expression Recognition) dataset.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Import Required Libraries\n",
        "\n",
        "# %%\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Load Dataset from Kaggle\n",
        "\n",
        "# %%\n",
        "# Download dataset from Kaggle\n",
        "print(\"Downloading Emotion Detection FER dataset from Kaggle...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"ananthu017/emotion-detection-fer\")\n",
        "    print(\"Dataset downloaded to:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Using fallback method...\")\n",
        "    path = \"./emotion-detection-fer\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# %%\n",
        "# Explore the directory structure\n",
        "def explore_directory_structure(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 2 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for f in files[:5]:  # Show only first 5 files\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "explore_directory_structure(path)\n",
        "\n",
        "# %%\n",
        "# Find data files and directories\n",
        "def find_image_directories(directory):\n",
        "    image_dirs = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        # Check if this directory contains image files\n",
        "        image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if image_files:\n",
        "            image_dirs.append(root)\n",
        "    return image_dirs\n",
        "\n",
        "def find_data_files(directory):\n",
        "    data_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv') or file.endswith('.json') or file.endswith('.xlsx'):\n",
        "                data_files.append(os.path.join(root, file))\n",
        "    return data_files\n",
        "\n",
        "image_dirs = find_image_directories(path)\n",
        "data_files = find_data_files(path)\n",
        "\n",
        "print(f\"Found image directories: {image_dirs}\")\n",
        "print(f\"Found data files: {data_files}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Data Loading and Exploration\n",
        "\n",
        "# %%\n",
        "# Load dataset information\n",
        "def load_emotion_data(image_dirs, data_files):\n",
        "    # First, try to load from CSV files\n",
        "    for file_path in data_files:\n",
        "        try:\n",
        "            if file_path.endswith('.csv'):\n",
        "                df = pd.read_csv(file_path)\n",
        "                print(f\"Successfully loaded metadata: {file_path}\")\n",
        "                print(f\"Metadata shape: {df.shape}\")\n",
        "                return df, image_dirs\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    # If no CSV found, create dataset structure from image directories\n",
        "    print(\"Creating dataset structure from image directories...\")\n",
        "\n",
        "    # Collect all images and their labels\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "\n",
        "    for img_dir in image_dirs:\n",
        "        # Try to infer label from directory name\n",
        "        dir_name = os.path.basename(img_dir)\n",
        "        if dir_name.lower() in ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']:\n",
        "            label = dir_name.lower()\n",
        "            if label not in label_map:\n",
        "                label_map[label] = len(label_map)\n",
        "\n",
        "            # Get all images in this directory\n",
        "            for file in os.listdir(img_dir):\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_paths.append(os.path.join(img_dir, file))\n",
        "                    labels.append(label)\n",
        "\n",
        "    if not image_paths:\n",
        "        print(\"No images found. Creating sample dataset structure...\")\n",
        "        return create_sample_emotion_data(), image_dirs\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'image_path': image_paths,\n",
        "        'emotion': labels\n",
        "    })\n",
        "\n",
        "    print(f\"Created dataset with {len(df)} images\")\n",
        "    print(f\"Emotion distribution:\\n{df['emotion'].value_counts()}\")\n",
        "\n",
        "    return df, image_dirs\n",
        "\n",
        "def create_sample_emotion_data():\n",
        "    \"\"\"Create sample emotion data structure\"\"\"\n",
        "    emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "    # Create sample DataFrame structure\n",
        "    sample_data = {\n",
        "        'emotion': emotions * 100,  # 700 samples\n",
        "        'pixels': [''] * 700,  # Placeholder\n",
        "        'Usage': ['Training'] * 700\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(sample_data)\n",
        "\n",
        "# Load the data\n",
        "df, image_dirs = load_emotion_data(image_dirs, data_files)\n",
        "print(\"\\nDataset overview:\")\n",
        "print(df.head())\n",
        "print(f\"\\nUnique emotions: {df['emotion'].unique()}\")\n",
        "print(f\"Dataset size: {len(df)}\")\n",
        "\n",
        "# %%\n",
        "# Explore emotion distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "emotion_counts = df['emotion'].value_counts()\n",
        "plt.bar(emotion_counts.index, emotion_counts.values)\n",
        "plt.title('Emotion Distribution')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(emotion_counts.values, labels=emotion_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Emotion Proportion')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Data Preprocessing and Image Loading\n",
        "\n",
        "# %%\n",
        "# Define emotion mapping\n",
        "emotion_mapping = {\n",
        "    'angry': 0,\n",
        "    'disgust': 1,\n",
        "    'fear': 2,\n",
        "    'happy': 3,\n",
        "    'neutral': 4,\n",
        "    'sad': 5,\n",
        "    'surprise': 6\n",
        "}\n",
        "\n",
        "reverse_emotion_mapping = {v: k for k, v in emotion_mapping.items()}\n",
        "\n",
        "def load_and_preprocess_images(df, image_size=(48, 48)):\n",
        "    \"\"\"Load and preprocess images from paths or pixel data\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            # Check if we have image paths or pixel data\n",
        "            if 'image_path' in df.columns and os.path.exists(row['image_path']):\n",
        "                # Load from image file\n",
        "                img = cv2.imread(row['image_path'])\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            elif 'pixels' in df.columns and row['pixels']:\n",
        "                # Load from pixel string (common in FER datasets)\n",
        "                pixel_list = list(map(int, row['pixels'].split()))\n",
        "                img = np.array(pixel_list, dtype=np.uint8).reshape(48, 48)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Resize image\n",
        "            img = cv2.resize(img, image_size)\n",
        "\n",
        "            # Normalize pixel values\n",
        "            img = img.astype('float32') / 255.0\n",
        "\n",
        "            # Add channel dimension\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            images.append(img)\n",
        "\n",
        "            # Get label\n",
        "            emotion = row['emotion']\n",
        "            labels.append(emotion_mapping[emotion])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images\n",
        "print(\"Loading and preprocessing images...\")\n",
        "X, y = load_and_preprocess_images(df)\n",
        "print(f\"Loaded {len(X)} images\")\n",
        "print(f\"Image shape: {X[0].shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "\n",
        "# %%\n",
        "# Visualize sample images\n",
        "def plot_sample_images(X, y, num_samples=12):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(X[i].squeeze(), cmap='gray')\n",
        "        plt.title(f'Emotion: {reverse_emotion_mapping[y[i]]}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Sample images from dataset:\")\n",
        "plot_sample_images(X, y)\n",
        "\n",
        "# %%\n",
        "# Convert labels to categorical\n",
        "y_categorical = to_categorical(y, num_classes=len(emotion_mapping))\n",
        "print(f\"Categorical labels shape: {y_categorical.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_categorical, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=np.argmax(y_temp, axis=1)\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Data Augmentation\n",
        "\n",
        "# %%\n",
        "# Create data generators with augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit the data generator\n",
        "datagen.fit(X_train)\n",
        "\n",
        "print(\"Data augmentation configured successfully\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Model Building - CNN Architecture\n",
        "\n",
        "# %%\n",
        "def build_emotion_cnn(input_shape=(48, 48, 1), num_classes=7):\n",
        "    \"\"\"Build CNN model for emotion detection\"\"\"\n",
        "    model = Sequential([\n",
        "        # First Convolutional Block\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Fourth Convolutional Block\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "model = build_emotion_cnn()\n",
        "model.summary()\n",
        "\n",
        "# %%\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', 'precision', 'recall']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Model Training\n",
        "\n",
        "# %%\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_emotion_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Train the model\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    epochs=100,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Model Evaluation\n",
        "\n",
        "# %%\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# Load best model\n",
        "model.load_weights('best_emotion_model.h5')\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# %%\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes,\n",
        "                          target_names=[reverse_emotion_mapping[i] for i in range(7)]))\n",
        "\n",
        "# %%\n",
        "# Confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[reverse_emotion_mapping[i] for i in range(7)],\n",
        "            yticklabels=[reverse_emotion_mapping[i] for i in range(7)])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Visualization of Predictions\n",
        "\n",
        "# %%\n",
        "# Plot sample predictions\n",
        "def plot_predictions(X, y_true, y_pred, num_samples=12):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(4, 3, i + 1)\n",
        "        plt.imshow(X[i].squeeze(), cmap='gray')\n",
        "\n",
        "        true_emotion = reverse_emotion_mapping[y_true[i]]\n",
        "        pred_emotion = reverse_emotion_mapping[y_pred[i]]\n",
        "        confidence = np.max(y_pred[i])\n",
        "\n",
        "        color = 'green' if true_emotion == pred_emotion else 'red'\n",
        "        plt.title(f'True: {true_emotion}\\nPred: {pred_emotion}\\nConf: {confidence:.2f}',\n",
        "                 color=color, fontsize=10)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get test set predictions for visualization\n",
        "test_predictions = model.predict(X_test)\n",
        "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "print(\"Sample predictions on test set:\")\n",
        "plot_predictions(X_test, y_true_classes, test_pred_classes)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Real-time Emotion Detection Function\n",
        "\n",
        "# %%\n",
        "def detect_emotion_live(image, model, emotion_map):\n",
        "    \"\"\"Detect emotion from a single image\"\"\"\n",
        "    # Preprocess image\n",
        "    if len(image.shape) == 3:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    image = cv2.resize(image, (48, 48))\n",
        "    image = image.astype('float32') / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(image, verbose=0)\n",
        "    emotion_idx = np.argmax(prediction)\n",
        "    confidence = np.max(prediction)\n",
        "    emotion = emotion_map[emotion_idx]\n",
        "\n",
        "    return emotion, confidence, prediction[0]\n",
        "\n",
        "# Test the function with sample images\n",
        "print(\"Testing emotion detection on sample images:\")\n",
        "for i in range(3):\n",
        "    sample_img = X_test[i]\n",
        "    emotion, confidence, all_probs = detect_emotion_live(\n",
        "        sample_img.squeeze(), model, reverse_emotion_mapping\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(sample_img.squeeze(), cmap='gray')\n",
        "    plt.title(f'Detected: {emotion} (Conf: {confidence:.2f})')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    emotions_list = [reverse_emotion_mapping[i] for i in range(7)]\n",
        "    plt.barh(emotions_list, all_probs)\n",
        "    plt.xlabel('Probability')\n",
        "    plt.title('Emotion Probabilities')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Model Interpretation with Grad-CAM\n",
        "\n",
        "# %%\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    \"\"\"Generate Grad-CAM heatmap for model interpretation\"\"\"\n",
        "    # Create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# Apply Grad-CAM to sample images\n",
        "print(\"Generating Grad-CAM visualizations...\")\n",
        "last_conv_layer_name = \"conv2d_7\"  # Adjust based on your model summary\n",
        "\n",
        "for i in range(3):\n",
        "    sample_img = X_test[i:i+1]\n",
        "    heatmap = make_gradcam_heatmap(sample_img, model, last_conv_layer_name)\n",
        "\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = plt.colormaps.get_cmap(\"jet\")\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((sample_img.shape[2], sample_img.shape[1]))\n",
        "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * 0.4 + sample_img[0] * 255 * 0.6\n",
        "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(sample_img[0].squeeze(), cmap='gray')\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(heatmap, cmap='jet')\n",
        "    plt.title('Heatmap')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.title('Superimposed')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 12. Model Saving and Deployment\n",
        "\n",
        "# %%\n",
        "# Save the final model\n",
        "model.save('emotion_detection_model.h5')\n",
        "print(\"Model saved as 'emotion_detection_model.h5'\")\n",
        "\n",
        "# Save the emotion mapping\n",
        "import json\n",
        "with open('emotion_mapping.json', 'w') as f:\n",
        "    json.dump(emotion_mapping, f)\n",
        "print(\"Emotion mapping saved as 'emotion_mapping.json'\")\n",
        "\n",
        "# Save model summary\n",
        "with open('model_architecture.txt', 'w') as f:\n",
        "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "print(\"Model architecture saved as 'model_architecture.txt'\")\n",
        "\n",
        "# Create performance report\n",
        "performance_report = {\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'test_precision': float(test_precision),\n",
        "    'test_recall': float(test_recall),\n",
        "    'test_loss': float(test_loss),\n",
        "    'training_samples': len(X_train),\n",
        "    'validation_samples': len(X_val),\n",
        "    'test_samples': len(X_test),\n",
        "    'input_shape': X_train[0].shape,\n",
        "    'num_classes': len(emotion_mapping),\n",
        "    'class_distribution': df['emotion'].value_counts().to_dict()\n",
        "}\n",
        "\n",
        "with open('performance_report.json', 'w') as f:\n",
        "    json.dump(performance_report, f, indent=2)\n",
        "print(\"Performance report saved as 'performance_report.json'\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 13. Real-time Webcam Emotion Detection (Optional)\n",
        "\n",
        "# %%\n",
        "# Optional: Webcam emotion detection\n",
        "def webcam_emotion_detection():\n",
        "    \"\"\"Real-time emotion detection using webcam\"\"\"\n",
        "    try:\n",
        "        # Initialize webcam\n",
        "        cap = cv2.VideoCapture(0)\n",
        "\n",
        "        # Load face detection classifier\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "        print(\"Starting webcam emotion detection...\")\n",
        "        print(\"Press 'q' to quit\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert to grayscale for face detection\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Detect faces\n",
        "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "            for (x, y, w, h) in faces:\n",
        "                # Extract face ROI\n",
        "                face_roi = gray[y:y+h, x:x+w]\n",
        "\n",
        "                # Detect emotion\n",
        "                emotion, confidence, _ = detect_emotion_live(face_roi, model, reverse_emotion_mapping)\n",
        "\n",
        "                # Draw rectangle and label\n",
        "                color = (0, 255, 0) if confidence > 0.6 else (0, 0, 255)\n",
        "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
        "                cv2.putText(frame, f'{emotion} ({confidence:.2f})',\n",
        "                           (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "            # Display frame\n",
        "            cv2.imshow('Emotion Detection', frame)\n",
        "\n",
        "            # Break on 'q' press\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # Cleanup\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Webcam error: {e}\")\n",
        "        print(\"Webcam functionality may not be available in this environment\")\n",
        "\n",
        "# Uncomment the following line to try webcam emotion detection\n",
        "# webcam_emotion_detection()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "#\n",
        "# This notebook demonstrates a complete pipeline for emotion detection from facial expressions:\n",
        "# - **Automatic dataset loading** from Kaggle with fallback options\n",
        "# - **Comprehensive EDA** and data visualization\n",
        "# - **Advanced CNN architecture** with batch normalization and dropout\n",
        "# - **Data augmentation** for improved generalization\n",
        "# - **Model interpretation** using Grad-CAM\n",
        "# - **Real-time emotion detection** capabilities\n",
        "#\n",
        "# The model can detect 7 emotions: angry, disgust, fear, happy, neutral, sad, surprise\n",
        "#\n",
        "# ### Further Improvements:\n",
        "# - Use larger datasets like AffectNet or RAF-DB\n",
        "# - Implement ensemble methods\n",
        "# - Add attention mechanisms\n",
        "# - Incorporate temporal information for video analysis\n",
        "# - Use transfer learning with pre-trained models"
      ],
      "metadata": {
        "id": "xsYe6XC8mH3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "WNuc0na3rhsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Intelligent ChatBot System\n",
        "# ## Complete NLP and Deep Learning Pipeline\n",
        "#\n",
        "# This notebook implements a comprehensive chatbot system using deep learning and natural language processing techniques.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Import Required Libraries\n",
        "\n",
        "# %%\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Deep Learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Dropout, Bidirectional, Input, GlobalMaxPool1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Load Dataset from Kaggle\n",
        "\n",
        "# %%\n",
        "# Download dataset from Kaggle\n",
        "print(\"Downloading ChatBot dataset from Kaggle...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"ahmedmoabdelkader/my-chatbot\")\n",
        "    print(\"Dataset downloaded to:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Using fallback method...\")\n",
        "    path = \"./chatbot-data\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# %%\n",
        "# Explore the directory structure\n",
        "def explore_directory_structure(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 2 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for f in files[:5]:  # Show only first 5 files\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "explore_directory_structure(path)\n",
        "\n",
        "# %%\n",
        "# Find data files\n",
        "def find_data_files(directory):\n",
        "    data_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(('.csv', '.json', '.xlsx', '.txt', '.pkl')):\n",
        "                data_files.append(os.path.join(root, file))\n",
        "    return data_files\n",
        "\n",
        "data_files = find_data_files(path)\n",
        "print(f\"Found data files: {data_files}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Data Loading and Exploration\n",
        "\n",
        "# %%\n",
        "def load_chatbot_data(data_files):\n",
        "    \"\"\"Load chatbot data from various file formats\"\"\"\n",
        "    # Try to load from different file types\n",
        "    for file_path in data_files:\n",
        "        try:\n",
        "            if file_path.endswith('.json'):\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                print(f\"Successfully loaded JSON: {file_path}\")\n",
        "                return data\n",
        "            elif file_path.endswith('.csv'):\n",
        "                df = pd.read_csv(file_path)\n",
        "                print(f\"Successfully loaded CSV: {file_path}\")\n",
        "                print(f\"Dataset shape: {df.shape}\")\n",
        "                return df\n",
        "            elif file_path.endswith('.xlsx'):\n",
        "                df = pd.read_excel(file_path)\n",
        "                print(f\"Successfully loaded Excel: {file_path}\")\n",
        "                print(f\"Dataset shape: {df.shape}\")\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    # If no file loaded successfully, create sample data\n",
        "    print(\"Creating sample chatbot data for demonstration...\")\n",
        "    return create_sample_chatbot_data()\n",
        "\n",
        "def create_sample_chatbot_data():\n",
        "    \"\"\"Create sample chatbot training data\"\"\"\n",
        "    sample_data = {\n",
        "        \"intents\": [\n",
        "            {\n",
        "                \"tag\": \"greeting\",\n",
        "                \"patterns\": [\n",
        "                    \"Hello\", \"Hi\", \"Hey\", \"How are you\", \"Is anyone there?\",\n",
        "                    \"Good day\", \"What's up\", \"How's it going\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"Hello! How can I help you today?\",\n",
        "                    \"Hi there! What can I do for you?\",\n",
        "                    \"Hey! Nice to see you. How can I assist?\",\n",
        "                    \"Hello! I'm here to help. What do you need?\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"tag\": \"goodbye\",\n",
        "                \"patterns\": [\n",
        "                    \"Bye\", \"Goodbye\", \"See you later\", \"Nice talking to you\",\n",
        "                    \"I'm leaving\", \"Have a good day\", \"Take care\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"Goodbye! Have a great day!\",\n",
        "                    \"See you later! Come back anytime.\",\n",
        "                    \"Take care! Feel free to chat again.\",\n",
        "                    \"Bye! It was nice talking to you.\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"tag\": \"thanks\",\n",
        "                \"patterns\": [\n",
        "                    \"Thanks\", \"Thank you\", \"That's helpful\", \"Awesome thanks\",\n",
        "                    \"Thanks for the help\", \"Thank you so much\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"You're welcome!\",\n",
        "                    \"Happy to help!\",\n",
        "                    \"Anytime! That's what I'm here for.\",\n",
        "                    \"Glad I could assist you!\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"tag\": \"about\",\n",
        "                \"patterns\": [\n",
        "                    \"Who are you?\", \"What are you?\", \"Tell me about yourself\",\n",
        "                    \"What can you do?\", \"What is your purpose?\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"I'm an AI chatbot designed to help and chat with you!\",\n",
        "                    \"I'm your friendly assistant chatbot. I can answer questions and have conversations.\",\n",
        "                    \"I'm a chatbot created to assist you with various tasks and have friendly conversations.\",\n",
        "                    \"I'm here to help you! I can answer questions, provide information, and chat with you.\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"tag\": \"name\",\n",
        "                \"patterns\": [\n",
        "                    \"What is your name?\", \"Who am I talking to?\", \"What should I call you?\",\n",
        "                    \"Do you have a name?\", \"What's your name?\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"You can call me ChatBot!\",\n",
        "                    \"I'm ChatBot, your virtual assistant.\",\n",
        "                    \"My name is ChatBot. Nice to meet you!\",\n",
        "                    \"I'm known as ChatBot. How can I help you?\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"tag\": \"help\",\n",
        "                \"patterns\": [\n",
        "                    \"Help\", \"I need help\", \"Can you help me?\", \"What can you do?\",\n",
        "                    \"How does this work?\", \"I need assistance\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"I can help you with various tasks! Try asking me about different topics.\",\n",
        "                    \"I'm here to assist you! You can ask me questions or we can just chat.\",\n",
        "                    \"I can answer questions, provide information, and have conversations. What would you like to know?\",\n",
        "                    \"I'm designed to help with information and conversation. What do you need help with?\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"tag\": \"weather\",\n",
        "                \"patterns\": [\n",
        "                    \"What's the weather like?\", \"How's the weather today?\",\n",
        "                    \"Is it going to rain?\", \"Weather forecast\", \"Temperature today\"\n",
        "                ],\n",
        "                \"responses\": [\n",
        "                    \"I don't have real-time weather data, but you can check your local weather app!\",\n",
        "                    \"For accurate weather information, please check a weather service in your area.\",\n",
        "                    \"I recommend checking a reliable weather source for current conditions.\",\n",
        "                    \"Weather updates are best obtained from dedicated weather services.\"\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    return sample_data\n",
        "\n",
        "# Load the data\n",
        "data = load_chatbot_data(data_files)\n",
        "print(\"\\nData type:\", type(data))\n",
        "\n",
        "# Display data structure\n",
        "if isinstance(data, dict) and 'intents' in data:\n",
        "    print(f\"Number of intent categories: {len(data['intents'])}\")\n",
        "    for intent in data['intents']:\n",
        "        print(f\"\\nTag: {intent['tag']}\")\n",
        "        print(f\"Patterns: {len(intent['patterns'])}\")\n",
        "        print(f\"Responses: {len(intent['responses'])}\")\n",
        "        print(f\"Sample pattern: {intent['patterns'][0]}\")\n",
        "        print(f\"Sample response: {intent['responses'][0]}\")\n",
        "elif isinstance(data, pd.DataFrame):\n",
        "    print(\"DataFrame columns:\", data.columns.tolist())\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(data.head())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Data Preprocessing - IMPROVED\n",
        "\n",
        "# %%\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Text preprocessing class for chatbot data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text - IMPROVED\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Keep basic punctuation and words, remove special characters but keep important symbols\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\?\\!\\.\\']', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text\"\"\"\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove stopwords from tokens - LESS AGGRESSIVE\"\"\"\n",
        "        # Keep some important stopwords that carry meaning in conversations\n",
        "        important_stopwords = {'what', 'how', 'who', 'when', 'where', 'why', 'which', 'can', 'could', 'would', 'should', 'will', 'is', 'are', 'am'}\n",
        "        return [token for token in tokens if token not in self.stop_words or token in important_stopwords]\n",
        "\n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        \"\"\"Apply lemmatization to tokens\"\"\"\n",
        "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    def full_preprocess(self, text, use_lemmatization=True):\n",
        "        \"\"\"Complete text preprocessing pipeline - IMPROVED\"\"\"\n",
        "        # Clean text\n",
        "        cleaned_text = self.clean_text(text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = self.tokenize(cleaned_text)\n",
        "\n",
        "        # Remove stopwords (less aggressive)\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "\n",
        "        # Apply lemmatization\n",
        "        tokens = self.lemmatize_tokens(tokens)\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# %%\n",
        "# Prepare training data - IMPROVED\n",
        "def prepare_training_data(data):\n",
        "    \"\"\"Prepare training data from the loaded dataset - IMPROVED\"\"\"\n",
        "    patterns = []\n",
        "    tags = []\n",
        "    responses = {}\n",
        "\n",
        "    if isinstance(data, dict) and 'intents' in data:\n",
        "        # JSON format data\n",
        "        for intent in data['intents']:\n",
        "            tag = intent['tag']\n",
        "            for pattern in intent['patterns']:\n",
        "                # Preprocess pattern - less aggressive preprocessing\n",
        "                processed_pattern = preprocessor.clean_text(pattern)  # Only clean, don't remove too much\n",
        "                if processed_pattern.strip():  # Only add non-empty patterns\n",
        "                    patterns.append(processed_pattern)\n",
        "                    tags.append(tag)\n",
        "\n",
        "            # Store responses for each tag\n",
        "            responses[tag] = intent['responses']\n",
        "\n",
        "    # Add more variations to improve training\n",
        "    additional_patterns = []\n",
        "    additional_tags = []\n",
        "\n",
        "    # Add common variations\n",
        "    variations = {\n",
        "        'greeting': ['hi', 'hello', 'hey', 'hi there', 'hello there', 'hey there', 'good morning', 'good afternoon', 'good evening'],\n",
        "        'goodbye': ['bye', 'goodbye', 'see you', 'see ya', 'take care', 'farewell', 'cya'],\n",
        "        'thanks': ['thanks', 'thank you', 'thx', 'thank you so much', 'thanks a lot', 'appreciate it'],\n",
        "        'about': ['who are you', 'what are you', 'tell me about you', 'what do you do', 'what can you do'],\n",
        "        'name': ['your name', 'whats your name', 'who am i talking to', 'what should i call you'],\n",
        "        'help': ['help', 'help me', 'i need help', 'can you help', 'assistance', 'support'],\n",
        "        'weather': ['weather', 'forecast', 'temperature', 'how is weather', 'weather today'],\n",
        "        'joke': ['joke', 'tell joke', 'make me laugh', 'funny', 'entertain me'],\n",
        "        'time': ['time', 'what time', 'current time', 'time now'],\n",
        "        'feelings': ['how are you', 'how do you feel', 'are you ok', 'you good', 'your mood']\n",
        "    }\n",
        "\n",
        "    for tag, pattern_list in variations.items():\n",
        "        if tag in responses:  # Only add variations for tags we have\n",
        "            for pattern in pattern_list:\n",
        "                patterns.append(pattern)\n",
        "                tags.append(tag)\n",
        "\n",
        "    return patterns, tags, responses\n",
        "\n",
        "# Prepare the data\n",
        "patterns, tags, responses = prepare_training_data(data)\n",
        "\n",
        "print(f\"Total patterns: {len(patterns)}\")\n",
        "print(f\"Total tags: {len(tags)}\")\n",
        "print(f\"Unique tags: {set(tags)}\")\n",
        "print(f\"\\nSample patterns: {patterns[:5]}\")\n",
        "print(f\"Sample tags: {tags[:5]}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Text Vectorization - IMPROVED\n",
        "\n",
        "# %%\n",
        "# Tokenize and pad sequences - IMPROVED\n",
        "tokenizer = Tokenizer(num_words=2000, oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(patterns)\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(patterns)\n",
        "\n",
        "# Pad sequences with appropriate length\n",
        "pattern_lengths = [len(pattern.split()) for pattern in patterns]\n",
        "max_length = max(pattern_lengths) + 5 if pattern_lengths else 15  # Add more padding\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "print(f\"Padded sequences shape: {X.shape}\")\n",
        "print(f\"Vocabulary size from tokenizer: {len(tokenizer.word_index)}\")\n",
        "print(f\"Most common words: {list(tokenizer.word_index.items())[:10]}\")\n",
        "\n",
        "# %%\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(tags)\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names: {label_encoder.classes_}\")\n",
        "\n",
        "# Convert to categorical for neural network\n",
        "y_categorical = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y_categorical.shape}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Model Building - IMPROVED\n",
        "\n",
        "# %%\n",
        "def build_chatbot_model(vocab_size, max_length, num_classes):\n",
        "    \"\"\"Build improved LSTM-based chatbot model\"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
        "\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Bidirectional(LSTM(32)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(16, activation='relu'),\n",
        "\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model = build_chatbot_model(vocab_size, max_length, num_classes)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Model Training - IMPROVED\n",
        "\n",
        "# %%\n",
        "# Split the data with proper stratification\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calculate class weights to handle imbalanced data\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y),\n",
        "    y=y\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "if len(X) > 1:\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y_categorical, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=np.argmax(y_temp, axis=1)\n",
        "    )\n",
        "else:\n",
        "    # If we have very little data, use all for training\n",
        "    X_train, y_train = X, y_categorical\n",
        "    X_val, y_val = X, y_categorical\n",
        "    X_test, y_test = X, y_categorical\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "# %%\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=20,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    min_lr=0.0001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_chatbot_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Train the model with more epochs and class weights\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=16,  # Smaller batch size for better learning\n",
        "    epochs=150,     # More epochs\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
        "    class_weight=class_weight_dict,  # Use class weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. ChatBot Implementation - IMPROVED\n",
        "\n",
        "# %%\n",
        "class IntelligentChatBot:\n",
        "    \"\"\"Intelligent ChatBot class with improved response handling\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, label_encoder, responses, preprocessor, max_length):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_encoder = label_encoder\n",
        "        self.responses = responses\n",
        "        self.preprocessor = preprocessor\n",
        "        self.max_length = max_length\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def preprocess_input(self, text):\n",
        "        \"\"\"Preprocess user input - LESS AGGRESSIVE\"\"\"\n",
        "        # Use lighter preprocessing for better recognition\n",
        "        text = text.lower().strip()\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\?\\!\\.\\']', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def predict_intent(self, text):\n",
        "        \"\"\"Predict intent from user input\"\"\"\n",
        "        try:\n",
        "            # Preprocess text\n",
        "            processed_text = self.preprocess_input(text)\n",
        "\n",
        "            # Convert to sequence\n",
        "            sequence = self.tokenizer.texts_to_sequences([processed_text])\n",
        "\n",
        "            if not sequence or not sequence[0]:\n",
        "                return \"unknown\", 0.0, np.array([0])\n",
        "\n",
        "            # Pad sequence\n",
        "            padded_sequence = pad_sequences(sequence, maxlen=self.max_length, padding='post')\n",
        "\n",
        "            # Predict\n",
        "            prediction = self.model.predict(padded_sequence, verbose=0)\n",
        "            intent_idx = np.argmax(prediction[0])\n",
        "            confidence = np.max(prediction[0])\n",
        "            intent = self.label_encoder.inverse_transform([intent_idx])[0]\n",
        "\n",
        "            return intent, confidence, prediction[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error: {e}\")\n",
        "            return \"unknown\", 0.0, np.array([0])\n",
        "\n",
        "    def get_response(self, intent, confidence):\n",
        "        \"\"\"Get response for predicted intent - LOWER CONFIDENCE THRESHOLD\"\"\"\n",
        "        confidence_threshold = 0.1  # Lower threshold for better responses\n",
        "        if intent in self.responses and confidence >= confidence_threshold:\n",
        "            return random.choice(self.responses[intent])\n",
        "        else:\n",
        "            return self.get_fallback_response()\n",
        "\n",
        "    def get_fallback_response(self):\n",
        "        \"\"\"Get better fallback responses\"\"\"\n",
        "        fallback_responses = [\n",
        "            \"I'm not sure I understand. Could you try asking differently?\",\n",
        "            \"That's an interesting question! Can you rephrase it?\",\n",
        "            \"I'm still learning. Could you ask me something else?\",\n",
        "            \"I don't have information about that yet. Try asking about greetings, help, or general questions.\",\n",
        "            \"Let me think... Could you try a different question?\"\n",
        "        ]\n",
        "        return random.choice(fallback_responses)\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment of user input\"\"\"\n",
        "        try:\n",
        "            blob = TextBlob(text)\n",
        "            sentiment = blob.sentiment.polarity\n",
        "\n",
        "            if sentiment > 0.1:\n",
        "                return \"positive\", sentiment\n",
        "            elif sentiment < -0.1:\n",
        "                return \"negative\", sentiment\n",
        "            else:\n",
        "                return \"neutral\", sentiment\n",
        "        except:\n",
        "            return \"neutral\", 0.0\n",
        "\n",
        "    def chat(self, user_input):\n",
        "        \"\"\"Main chat method with better intent matching\"\"\"\n",
        "        # Analyze sentiment\n",
        "        sentiment, sentiment_score = self.analyze_sentiment(user_input)\n",
        "\n",
        "        # Predict intent\n",
        "        intent, confidence, all_probs = self.predict_intent(user_input)\n",
        "\n",
        "        # Get response\n",
        "        response = self.get_response(intent, confidence)\n",
        "\n",
        "        # Store conversation\n",
        "        self.conversation_history.append({\n",
        "            'user_input': user_input,\n",
        "            'response': response,\n",
        "            'intent': intent,\n",
        "            'confidence': confidence,\n",
        "            'sentiment': sentiment,\n",
        "            'sentiment_score': sentiment_score\n",
        "        })\n",
        "\n",
        "        return response, intent, confidence, sentiment, sentiment_score\n",
        "\n",
        "    def get_conversation_summary(self):\n",
        "        \"\"\"Get conversation summary\"\"\"\n",
        "        if not self.conversation_history:\n",
        "            return \"No conversation history yet.\"\n",
        "\n",
        "        total_messages = len(self.conversation_history)\n",
        "        sentiments = [msg['sentiment'] for msg in self.conversation_history]\n",
        "        sentiment_counts = {sentiment: sentiments.count(sentiment) for sentiment in set(sentiments)}\n",
        "\n",
        "        summary = f\"Conversation Summary:\\n\"\n",
        "        summary += f\"Total messages: {total_messages}\\n\"\n",
        "        summary += f\"Sentiment distribution: {sentiment_counts}\\n\"\n",
        "        summary += f\"Recent intents: {[msg['intent'] for msg in self.conversation_history[-5:]]}\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "# Re-initialize chatbot with improved class\n",
        "chatbot = IntelligentChatBot(model, tokenizer, label_encoder, responses, preprocessor, max_length)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Test ChatBot Before Interactive Session\n",
        "\n",
        "# %%\n",
        "# Test the chatbot with common inputs\n",
        "test_inputs = [\n",
        "    \"hi\",\n",
        "    \"hello\",\n",
        "    \"hey\",\n",
        "    \"how are you?\",\n",
        "    \"what's your name?\",\n",
        "    \"help\",\n",
        "    \"thank you\",\n",
        "    \"bye\",\n",
        "    \"good morning\",\n",
        "    \"what can you do?\"\n",
        "]\n",
        "\n",
        "print(\"Testing chatbot with common inputs:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for test_input in test_inputs:\n",
        "    response, intent, confidence, sentiment, sentiment_score = chatbot.chat(test_input)\n",
        "    print(f\"Input: '{test_input}'\")\n",
        "    print(f\"Response: '{response}'\")\n",
        "    print(f\"Intent: {intent}, Confidence: {confidence:.2f}, Sentiment: {sentiment}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Only start interactive session if basic tests pass\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASIC TEST RESULTS:\")\n",
        "print(\"If confidence scores are above 0.3 and correct intents are detected,\")\n",
        "print(\"the chatbot should work well in interactive mode.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Interactive Chat Interface\n",
        "\n",
        "# %%\n",
        "def interactive_chat_session():\n",
        "    \"\"\"Run an interactive chat session\"\"\"\n",
        "    print(\"\\n Welcome to Intelligent ChatBot!\")\n",
        "    print(\"Type 'quit' to exit, 'summary' for conversation summary, 'help' for commands\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"You: \").strip()\n",
        "\n",
        "            if user_input.lower() == 'quit':\n",
        "                print(\"ChatBot: Goodbye! Thanks for chatting!\")\n",
        "                break\n",
        "            elif user_input.lower() == 'summary':\n",
        "                summary = chatbot.get_conversation_summary()\n",
        "                print(f\"ChatBot: {summary}\")\n",
        "                continue\n",
        "            elif user_input.lower() == 'help':\n",
        "                help_text = \"\"\"\n",
        "Available commands:\n",
        "- 'quit': Exit the chat\n",
        "- 'summary': Show conversation summary\n",
        "- 'help': Show this help message\n",
        "\n",
        "You can ask me about:\n",
        "- Greetings (hi, hello, hey)\n",
        "- My name and purpose\n",
        "- Help and assistance\n",
        "- Weather and time\n",
        "- Jokes\n",
        "- How I'm feeling\n",
        "- Or just say thanks or goodbye!\n",
        "                \"\"\"\n",
        "                print(help_text)\n",
        "                continue\n",
        "            elif not user_input:\n",
        "                print(\"ChatBot: Please type something!\")\n",
        "                continue\n",
        "\n",
        "            # Get chatbot response\n",
        "            response, intent, confidence, sentiment, sentiment_score = chatbot.chat(user_input)\n",
        "\n",
        "            # Display response with metadata\n",
        "            print(f\" ChatBot: {response}\")\n",
        "            if confidence < 0.3:\n",
        "                print(f\"   [Low confidence: {confidence:.2f} - I might not understand perfectly]\")\n",
        "            else:\n",
        "                print(f\"   [Intent: {intent}, Confidence: {confidence:.2f}, Sentiment: {sentiment}]\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nChatBot: Session interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"ChatBot: Sorry, I encountered an error. Please try again.\")\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Start interactive chat\n",
        "print(\"\\nStarting interactive chat session...\")\n",
        "interactive_chat_session()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 12. Model Saving and Deployment\n",
        "\n",
        "# %%\n",
        "# Save the trained model\n",
        "model.save('chatbot_model.h5')\n",
        "print(\"Model saved as 'chatbot_model.h5'\")\n",
        "\n",
        "# Save tokenizer\n",
        "import pickle\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"Tokenizer saved as 'tokenizer.pkl'\")\n",
        "\n",
        "# Save label encoder\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "print(\"Label encoder saved as 'label_encoder.pkl'\")\n",
        "\n",
        "# Save preprocessor\n",
        "with open('preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "print(\"Preprocessor saved as 'preprocessor.pkl'\")\n",
        "\n",
        "# Save responses\n",
        "with open('responses.pkl', 'wb') as f:\n",
        "    pickle.dump(responses, f)\n",
        "print(\"Responses saved as 'responses.pkl'\")\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    'max_length': max_length,\n",
        "    'vocab_size': vocab_size,\n",
        "    'num_classes': num_classes,\n",
        "    'class_names': label_encoder.classes_.tolist()\n",
        "}\n",
        "\n",
        "with open('chatbot_config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(\"Configuration saved as 'chatbot_config.json'\")\n",
        "\n",
        "# Create deployment package\n",
        "deployment_files = [\n",
        "    'chatbot_model.h5',\n",
        "    'tokenizer.pkl',\n",
        "    'label_encoder.pkl',\n",
        "    'preprocessor.pkl',\n",
        "    'responses.pkl',\n",
        "    'chatbot_config.json'\n",
        "]\n",
        "\n",
        "print(f\"\\nDeployment package created with {len(deployment_files)} files\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "#\n",
        "# This notebook demonstrates a complete intelligent chatbot system with:\n",
        "#\n",
        "# ### Key Features:\n",
        "# - **Automatic dataset loading** from Kaggle with fallback to sample data\n",
        "# - **Advanced text preprocessing** with stemming/lemmatization\n",
        "# - **Bidirectional LSTM model** for intent classification\n",
        "# - **Sentiment analysis** of user inputs\n",
        "# - **Interactive chat interface** with conversation history\n",
        "# - **Comprehensive analytics** and visualization\n",
        "# - **Model persistence** for deployment\n",
        "#\n",
        "# ### Capabilities:\n",
        "# - Intent recognition with confidence scoring\n",
        "# - Contextual response generation\n",
        "# - Sentiment analysis\n",
        "# - Conversation history tracking\n",
        "# - Fallback responses for unknown inputs\n",
        "#\n",
        "# ### Further Enhancements:\n",
        "# - Integration with external APIs (weather, news, etc.)\n",
        "# - Multi-turn conversation context\n",
        "# - User personalization\n",
        "# - Voice interface integration\n",
        "# - Multi-language support\n",
        "# - Integration with knowledge bases\n",
        "#\n",
        "# The chatbot is ready for deployment and can be easily extended with additional features!"
      ],
      "metadata": {
        "id": "gBv5a6-AyK2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "-RTUVVTD4SXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Automatic Number Plate Recognition (ANPR)\n",
        "# ## Complete License Plate Detection and Recognition System\n",
        "#\n",
        "# This notebook implements a comprehensive ANPR system using Computer Vision and Deep Learning techniques.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Installation and Setup\n",
        "\n",
        "# %%\n",
        "# Install required packages\n",
        "!pip install kagglehub pandas numpy matplotlib seaborn scikit-learn tensorflow opencv-python pillow\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install easyocr pytesseract imutils\n",
        "!apt-get install tesseract-ocr\n",
        "\n",
        "# %%\n",
        "import kagglehub\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import imutils\n",
        "import easyocr\n",
        "import pytesseract\n",
        "from skimage import filters\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"OpenCV version:\", cv2.__version__)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Load Dataset from Kaggle\n",
        "\n",
        "# %%\n",
        "# Download dataset from Kaggle\n",
        "print(\"Downloading License Plate dataset from Kaggle...\")\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"sarthakvajpayee/license-plate-recognition-using-cnn\")\n",
        "    print(\"Dataset downloaded to:\", path)\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Using fallback method...\")\n",
        "    path = \"./license-plate-data\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# %%\n",
        "# Explore the directory structure\n",
        "def explore_directory_structure(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 2 * (level)\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for f in files[:5]:  # Show only first 5 files\n",
        "            print(f\"{subindent}{f}\")\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "explore_directory_structure(path)\n",
        "\n",
        "# %%\n",
        "# Find data files\n",
        "def find_data_files(directory):\n",
        "    data_files = []\n",
        "    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif')\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(image_extensions) or file.endswith('.csv'):\n",
        "                data_files.append(os.path.join(root, file))\n",
        "    return data_files\n",
        "\n",
        "data_files = find_data_files(path)\n",
        "image_files = [f for f in data_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "csv_files = [f for f in data_files if f.endswith('.csv')]\n",
        "\n",
        "print(f\"Found {len(image_files)} image files\")\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "print(f\"Sample image files: {image_files[:3]}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Data Loading and Exploration\n",
        "\n",
        "# %%\n",
        "def load_anpr_data(image_files, csv_files):\n",
        "    \"\"\"Load ANPR dataset\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "\n",
        "    # Try to load from CSV if available\n",
        "    if csv_files:\n",
        "        for csv_file in csv_files:\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "                print(f\"Loaded CSV: {csv_file}\")\n",
        "                print(f\"CSV columns: {df.columns.tolist()}\")\n",
        "\n",
        "                # Look for image path and label columns\n",
        "                if 'image_path' in df.columns and 'license_plate' in df.columns:\n",
        "                    for _, row in df.iterrows():\n",
        "                        img_path = os.path.join(os.path.dirname(csv_file), row['image_path'])\n",
        "                        if os.path.exists(img_path):\n",
        "                            images.append(cv2.imread(img_path))\n",
        "                            labels.append(row['license_plate'])\n",
        "                            image_paths.append(img_path)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading CSV {csv_file}: {e}\")\n",
        "\n",
        "    # If no CSV or CSV loading failed, use image files directly\n",
        "    if not images:\n",
        "        print(\"Loading images directly...\")\n",
        "        for img_file in image_files[:100]:  # Limit for demonstration\n",
        "            try:\n",
        "                img = cv2.imread(img_file)\n",
        "                if img is not None:\n",
        "                    images.append(img)\n",
        "                    # Extract filename as label or use placeholder\n",
        "                    filename = os.path.basename(img_file)\n",
        "                    labels.append(filename.split('.')[0])\n",
        "                    image_paths.append(img_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_file}: {e}\")\n",
        "\n",
        "    # If still no images, create sample data\n",
        "    if not images:\n",
        "        print(\"Creating sample license plate data...\")\n",
        "        return create_sample_anpr_data()\n",
        "\n",
        "    return images, labels, image_paths\n",
        "\n",
        "def create_sample_anpr_data():\n",
        "    \"\"\"Create sample license plate images for demonstration\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "\n",
        "    # Sample license plates\n",
        "    sample_plates = [\n",
        "        \"ABC123\", \"XYZ789\", \"DEF456\", \"GHI789\", \"JKL012\",\n",
        "        \"MNO345\", \"PQR678\", \"STU901\", \"VWX234\", \"YZA567\"\n",
        "    ]\n",
        "\n",
        "    for i, plate_text in enumerate(sample_plates):\n",
        "        # Create a simple license plate image\n",
        "        img = np.ones((100, 300, 3), dtype=np.uint8) * 255  # White background\n",
        "\n",
        "        # Add border\n",
        "        cv2.rectangle(img, (5, 5), (295, 95), (0, 0, 0), 2)\n",
        "\n",
        "        # Add text\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        text_size = cv2.getTextSize(plate_text, font, 2, 3)[0]\n",
        "        text_x = (300 - text_size[0]) // 2\n",
        "        text_y = (100 + text_size[1]) // 2\n",
        "        cv2.putText(img, plate_text, (text_x, text_y), font, 2, (0, 0, 0), 3)\n",
        "\n",
        "        images.append(img)\n",
        "        labels.append(plate_text)\n",
        "        image_paths.append(f\"sample_plate_{i}.jpg\")\n",
        "\n",
        "    return images, labels, image_paths\n",
        "\n",
        "# Load the data\n",
        "images, labels, image_paths = load_anpr_data(image_files, csv_files)\n",
        "\n",
        "print(f\"Loaded {len(images)} images\")\n",
        "print(f\"Sample labels: {labels[:5]}\")\n",
        "print(f\"Image shape: {images[0].shape}\")\n",
        "\n",
        "# %%\n",
        "# Display sample images\n",
        "def display_sample_images(images, labels, num_samples=6):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        # Convert BGR to RGB for display\n",
        "        img_rgb = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(img_rgb)\n",
        "        plt.title(f'Label: {labels[i]}')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Sample License Plate Images:\")\n",
        "display_sample_images(images, labels)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Image Preprocessing and License Plate Detection\n",
        "\n",
        "# %%\n",
        "class LicensePlateDetector:\n",
        "    \"\"\"License Plate Detection using Computer Vision techniques\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.min_area = 1000\n",
        "        self.max_area = 50000\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"Preprocess image for license plate detection\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply bilateral filter to reduce noise while keeping edges sharp\n",
        "        filtered = cv2.bilateralFilter(gray, 11, 17, 17)\n",
        "\n",
        "        # Apply edge detection\n",
        "        edged = cv2.Canny(filtered, 30, 200)\n",
        "\n",
        "        return gray, filtered, edged\n",
        "\n",
        "    def find_contours(self, edged):\n",
        "        \"\"\"Find contours in the edged image\"\"\"\n",
        "        # Find contours\n",
        "        contours = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        contours = imutils.grab_contours(contours)\n",
        "\n",
        "        # Sort contours by area in descending order\n",
        "        contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n",
        "\n",
        "        return contours\n",
        "\n",
        "    def find_license_plate_contour(self, contours):\n",
        "        \"\"\"Find the contour that resembles a license plate\"\"\"\n",
        "        license_plate_contour = None\n",
        "\n",
        "        for contour in contours:\n",
        "            # Approximate the contour\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            approx = cv2.approxPolyDP(contour, 0.018 * perimeter, True)\n",
        "\n",
        "            # Check if the contour has 4 vertices (rectangle)\n",
        "            if len(approx) == 4:\n",
        "                license_plate_contour = approx\n",
        "                break\n",
        "\n",
        "        return license_plate_contour\n",
        "\n",
        "    def extract_license_plate(self, image, contour):\n",
        "        \"\"\"Extract license plate region from image\"\"\"\n",
        "        if contour is None:\n",
        "            return None\n",
        "\n",
        "        # Get bounding rectangle\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract license plate region\n",
        "        license_plate = image[y:y+h, x:x+w]\n",
        "\n",
        "        return license_plate, (x, y, w, h)\n",
        "\n",
        "    def detect_plate(self, image):\n",
        "        \"\"\"Main method to detect license plate\"\"\"\n",
        "        try:\n",
        "            # Preprocess image\n",
        "            gray, filtered, edged = self.preprocess_image(image)\n",
        "\n",
        "            # Find contours\n",
        "            contours = self.find_contours(edged)\n",
        "\n",
        "            # Find license plate contour\n",
        "            plate_contour = self.find_license_plate_contour(contours)\n",
        "\n",
        "            if plate_contour is not None:\n",
        "                # Extract license plate\n",
        "                license_plate, bbox = self.extract_license_plate(image, plate_contour)\n",
        "                return license_plate, bbox, plate_contour\n",
        "            else:\n",
        "                return None, None, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in plate detection: {e}\")\n",
        "            return None, None, None\n",
        "\n",
        "# Initialize detector\n",
        "plate_detector = LicensePlateDetector()\n",
        "\n",
        "# %%\n",
        "# Test plate detection on sample images\n",
        "def test_plate_detection(images, num_samples=3):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        original_image = images[i].copy()\n",
        "\n",
        "        # Detect license plate\n",
        "        license_plate, bbox, contour = plate_detector.detect_plate(original_image)\n",
        "\n",
        "        # Plot original image with detection\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        img_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if contour is not None:\n",
        "            # Draw contour on original image\n",
        "            cv2.drawContours(img_rgb, [contour], -1, (0, 255, 0), 3)\n",
        "\n",
        "        plt.imshow(img_rgb)\n",
        "        plt.title(f'Detection Result {i+1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Plot extracted license plate if found\n",
        "        plt.subplot(2, 3, i + 4)\n",
        "        if license_plate is not None:\n",
        "            license_plate_rgb = cv2.cvtColor(license_plate, cv2.COLOR_BGR2RGB)\n",
        "            plt.imshow(license_plate_rgb)\n",
        "            plt.title('Extracted Plate')\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, 'No Plate Detected',\n",
        "                    horizontalalignment='center', verticalalignment='center',\n",
        "                    transform=plt.gca().transAxes, fontsize=12)\n",
        "            plt.gca().set_facecolor('lightgray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Testing License Plate Detection:\")\n",
        "test_plate_detection(images)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Character Segmentation\n",
        "\n",
        "# %%\n",
        "class CharacterSegmenter:\n",
        "    \"\"\"Segment individual characters from license plate\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.min_char_width = 10\n",
        "        self.min_char_height = 30\n",
        "        self.max_char_width = 100\n",
        "        self.max_char_height = 100\n",
        "\n",
        "    def preprocess_plate(self, plate_image):\n",
        "        \"\"\"Preprocess license plate for character segmentation\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply thresholding\n",
        "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        # Remove noise\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def segment_characters(self, plate_image):\n",
        "        \"\"\"Segment individual characters from license plate\"\"\"\n",
        "        try:\n",
        "            # Preprocess plate\n",
        "            processed_plate = self.preprocess_plate(plate_image)\n",
        "\n",
        "            # Find contours\n",
        "            contours, _ = cv2.findContours(processed_plate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            characters = []\n",
        "            bounding_boxes = []\n",
        "\n",
        "            for contour in contours:\n",
        "                # Get bounding box\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "                # Filter based on size (character dimensions)\n",
        "                if (self.min_char_width <= w <= self.max_char_width and\n",
        "                    self.min_char_height <= h <= self.max_char_height and\n",
        "                    h > w):  # Characters are typically taller than wide\n",
        "\n",
        "                    # Extract character\n",
        "                    char = processed_plate[y:y+h, x:x+w]\n",
        "\n",
        "                    # Resize to standard size\n",
        "                    char = cv2.resize(char, (32, 32))\n",
        "\n",
        "                    # Normalize\n",
        "                    char = char.astype('float32') / 255.0\n",
        "                    char = np.expand_dims(char, axis=-1)  # Add channel dimension\n",
        "\n",
        "                    characters.append(char)\n",
        "                    bounding_boxes.append((x, y, w, h))\n",
        "\n",
        "            # Sort characters from left to right\n",
        "            if characters and bounding_boxes:\n",
        "                sorted_indices = np.argsort([box[0] for box in bounding_boxes])\n",
        "                characters = [characters[i] for i in sorted_indices]\n",
        "                bounding_boxes = [bounding_boxes[i] for i in sorted_indices]\n",
        "\n",
        "            return characters, bounding_boxes, processed_plate\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in character segmentation: {e}\")\n",
        "            return [], [], None\n",
        "\n",
        "# Initialize segmenter\n",
        "char_segmenter = CharacterSegmenter()\n",
        "\n",
        "# %%\n",
        "# Test character segmentation\n",
        "def test_character_segmentation(images, num_samples=3):\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    sample_count = 0\n",
        "    for i in range(len(images)):\n",
        "        if sample_count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # Detect license plate first\n",
        "        license_plate, bbox, _ = plate_detector.detect_plate(images[i])\n",
        "\n",
        "        if license_plate is not None:\n",
        "            # Segment characters\n",
        "            characters, bboxes, processed_plate = char_segmenter.segment_characters(license_plate)\n",
        "\n",
        "            if characters:\n",
        "                # Plot original plate\n",
        "                plt.subplot(3, num_samples, sample_count + 1)\n",
        "                plate_rgb = cv2.cvtColor(license_plate, cv2.COLOR_BGR2RGB)\n",
        "                plt.imshow(plate_rgb)\n",
        "                plt.title('Original Plate')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Plot processed plate\n",
        "                plt.subplot(3, num_samples, sample_count + num_samples + 1)\n",
        "                plt.imshow(processed_plate, cmap='gray')\n",
        "                plt.title('Processed Plate')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Plot segmented characters\n",
        "                plt.subplot(3, num_samples, sample_count + 2 * num_samples + 1)\n",
        "                if len(characters) > 0:\n",
        "                    # Create montage of characters\n",
        "                    cols = min(8, len(characters))\n",
        "                    rows = (len(characters) + cols - 1) // cols\n",
        "                    montage = np.zeros((rows * 32, cols * 32), dtype=np.float32)\n",
        "\n",
        "                    for j, char in enumerate(characters):\n",
        "                        row = j // cols\n",
        "                        col = j % cols\n",
        "                        montage[row*32:(row+1)*32, col*32:(col+1)*32] = char.squeeze()\n",
        "\n",
        "                    plt.imshow(montage, cmap='gray')\n",
        "                    plt.title(f'{len(characters)} Characters')\n",
        "                else:\n",
        "                    plt.text(0.5, 0.5, 'No Characters',\n",
        "                            horizontalalignment='center', verticalalignment='center',\n",
        "                            transform=plt.gca().transAxes)\n",
        "                    plt.gca().set_facecolor('lightgray')\n",
        "                plt.axis('off')\n",
        "\n",
        "                sample_count += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Testing Character Segmentation:\")\n",
        "test_character_segmentation(images)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Character Recognition Model\n",
        "\n",
        "# %%\n",
        "def create_character_recognition_model(input_shape=(32, 32, 1), num_classes=36):\n",
        "    \"\"\"Create CNN model for character recognition\"\"\"\n",
        "    model = Sequential([\n",
        "        # First Convolutional Block\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Character classes: 0-9 and A-Z\n",
        "char_classes = [str(i) for i in range(10)] + [chr(i) for i in range(65, 91)]\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_classes)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_classes)}\n",
        "\n",
        "print(f\"Character classes: {char_classes}\")\n",
        "print(f\"Total classes: {len(char_classes)}\")\n",
        "\n",
        "# %%\n",
        "# Prepare training data for character recognition\n",
        "def prepare_character_training_data():\n",
        "    \"\"\"Prepare character training data (simplified for demo)\"\"\"\n",
        "    # In a real scenario, you would use a character dataset like EMNIST\n",
        "    # For demo purposes, we'll create simple synthetic data\n",
        "\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    # Create simple character images\n",
        "    for char_idx, char in enumerate(char_classes):\n",
        "        for _ in range(50):  # 50 samples per character\n",
        "            # Create a simple character image\n",
        "            img = np.ones((32, 32), dtype=np.float32) * 0.1  # Dark background\n",
        "\n",
        "            # Add random noise\n",
        "            img += np.random.normal(0, 0.1, (32, 32))\n",
        "\n",
        "            # Simulate character (simple representation)\n",
        "            center_x, center_y = 16, 16\n",
        "            if char.isdigit():\n",
        "                # Draw circle for digits\n",
        "                cv2.circle(img, (center_x, center_y), 8, (1.0, 1.0, 1.0), -1)\n",
        "            else:\n",
        "                # Draw rectangle for letters\n",
        "                cv2.rectangle(img, (center_x-8, center_y-8), (center_x+8, center_y+8),\n",
        "                             (1.0, 1.0, 1.0), -1)\n",
        "\n",
        "            X_train.append(img)\n",
        "            y_train.append(char_idx)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    # Add channel dimension\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "    # Convert to categorical\n",
        "    y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes=len(char_classes))\n",
        "\n",
        "    return X_train, y_train_categorical\n",
        "\n",
        "# Prepare data\n",
        "X_chars, y_chars = prepare_character_training_data()\n",
        "\n",
        "print(f\"Character training data shape: {X_chars.shape}\")\n",
        "print(f\"Character labels shape: {y_chars.shape}\")\n",
        "\n",
        "# %%\n",
        "# Build and train character recognition model\n",
        "char_model = create_character_recognition_model(num_classes=len(char_classes))\n",
        "\n",
        "char_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "char_model.summary()\n",
        "\n",
        "# %%\n",
        "# Train character recognition model\n",
        "char_history = char_model.fit(\n",
        "    X_chars, y_chars,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(char_history.history['loss'], label='Training Loss')\n",
        "plt.plot(char_history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Character Model Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(char_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(char_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Character Model Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Complete ANPR Pipeline\n",
        "\n",
        "# %%\n",
        "class ANPRSystem:\n",
        "    \"\"\"Complete Automatic Number Plate Recognition System\"\"\"\n",
        "\n",
        "    def __init__(self, plate_detector, char_segmenter, char_model, char_classes):\n",
        "        self.plate_detector = plate_detector\n",
        "        self.char_segmenter = char_segmenter\n",
        "        self.char_model = char_model\n",
        "        self.char_classes = char_classes\n",
        "\n",
        "        # Initialize OCR readers\n",
        "        self.easyocr_reader = easyocr.Reader(['en'])\n",
        "\n",
        "    def recognize_characters_cnn(self, characters):\n",
        "        \"\"\"Recognize characters using CNN model\"\"\"\n",
        "        recognized_chars = []\n",
        "        confidences = []\n",
        "\n",
        "        for char_img in characters:\n",
        "            # Prepare image for prediction\n",
        "            if len(char_img.shape) == 3:\n",
        "                char_img = char_img.squeeze()\n",
        "\n",
        "            # Resize to model input size\n",
        "            char_img = cv2.resize(char_img, (32, 32))\n",
        "            char_img = np.expand_dims(char_img, axis=-1)\n",
        "            char_img = np.expand_dims(char_img, axis=0)\n",
        "\n",
        "            # Predict\n",
        "            prediction = self.char_model.predict(char_img, verbose=0)\n",
        "            char_idx = np.argmax(prediction[0])\n",
        "            confidence = np.max(prediction[0])\n",
        "\n",
        "            recognized_chars.append(self.char_classes[char_idx])\n",
        "            confidences.append(confidence)\n",
        "\n",
        "        return recognized_chars, confidences\n",
        "\n",
        "    def recognize_characters_ocr(self, plate_image):\n",
        "        \"\"\"Recognize characters using OCR\"\"\"\n",
        "        try:\n",
        "            # Use EasyOCR for text recognition\n",
        "            results = self.easyocr_reader.readtext(plate_image)\n",
        "\n",
        "            if results:\n",
        "                # Get the result with highest confidence\n",
        "                best_result = max(results, key=lambda x: x[2])\n",
        "                text = best_result[1].upper()\n",
        "                confidence = best_result[2]\n",
        "\n",
        "                # Clean text (remove spaces and special characters)\n",
        "                text = ''.join(c for c in text if c.isalnum())\n",
        "\n",
        "                return text, confidence\n",
        "            else:\n",
        "                return \"\", 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"OCR error: {e}\")\n",
        "            return \"\", 0.0\n",
        "\n",
        "    def process_image(self, image):\n",
        "        \"\"\"Process complete image through ANPR pipeline\"\"\"\n",
        "        try:\n",
        "            # Step 1: Detect license plate\n",
        "            license_plate, bbox, contour = self.plate_detector.detect_plate(image)\n",
        "\n",
        "            if license_plate is None:\n",
        "                return None, None, None, \"No plate detected\"\n",
        "\n",
        "            # Step 2: Segment characters\n",
        "            characters, bboxes, processed_plate = self.char_segmenter.segment_characters(license_plate)\n",
        "\n",
        "            # Step 3: Recognize characters using multiple methods\n",
        "            plate_text_cnn = \"\"\n",
        "            plate_text_ocr = \"\"\n",
        "            confidence_cnn = 0.0\n",
        "            confidence_ocr = 0.0\n",
        "\n",
        "            # Method 1: CNN-based recognition\n",
        "            if characters:\n",
        "                recognized_chars, confidences = self.recognize_characters_cnn(characters)\n",
        "                plate_text_cnn = ''.join(recognized_chars)\n",
        "                confidence_cnn = np.mean(confidences) if confidences else 0.0\n",
        "\n",
        "            # Method 2: OCR-based recognition\n",
        "            plate_text_ocr, confidence_ocr = self.recognize_characters_ocr(license_plate)\n",
        "\n",
        "            # Choose the best result\n",
        "            if confidence_cnn > confidence_ocr:\n",
        "                final_text = plate_text_cnn\n",
        "                final_confidence = confidence_cnn\n",
        "                method = \"CNN\"\n",
        "            else:\n",
        "                final_text = plate_text_ocr\n",
        "                final_confidence = confidence_ocr\n",
        "                method = \"OCR\"\n",
        "\n",
        "            return final_text, final_confidence, method, license_plate\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ANPR processing error: {e}\")\n",
        "            return None, None, None, f\"Error: {e}\"\n",
        "\n",
        "# Initialize ANPR system\n",
        "anpr_system = ANPRSystem(plate_detector, char_segmenter, char_model, char_classes)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Testing and Evaluation\n",
        "\n",
        "# %%\n",
        "# Test the complete ANPR system\n",
        "def test_anpr_system(images, labels, num_samples=6):\n",
        "    plt.figure(figsize=(18, 12))\n",
        "\n",
        "    successful_detections = 0\n",
        "    sample_count = 0\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        if sample_count >= num_samples:\n",
        "            break\n",
        "\n",
        "        original_image = images[i].copy()\n",
        "\n",
        "        # Process image through ANPR system\n",
        "        plate_text, confidence, method, license_plate = anpr_system.process_image(original_image)\n",
        "\n",
        "        if plate_text:\n",
        "            # Plot results\n",
        "            plt.subplot(2, 3, sample_count + 1)\n",
        "\n",
        "            # Convert to RGB for display\n",
        "            img_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Add detection info to image\n",
        "            info_text = f\"Plate: {plate_text}\"\n",
        "            confidence_text = f\"Conf: {confidence:.2f} ({method})\"\n",
        "\n",
        "            cv2.putText(img_rgb, info_text, (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "            cv2.putText(img_rgb, confidence_text, (10, 60),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "            plt.imshow(img_rgb)\n",
        "            plt.title(f'Detection {sample_count + 1}')\n",
        "            plt.axis('off')\n",
        "\n",
        "            successful_detections += 1\n",
        "            sample_count += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Successfully processed {successful_detections} out of {num_samples} samples\")\n",
        "\n",
        "print(\"Testing Complete ANPR System:\")\n",
        "test_anpr_system(images, labels)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Real-time Processing Simulation\n",
        "\n",
        "# %%\n",
        "def simulate_real_time_processing(images, delay=2):\n",
        "    \"\"\"Simulate real-time ANPR processing\"\"\"\n",
        "    print(\" Starting Real-time ANPR Simulation...\")\n",
        "    print(\"Press Ctrl+C to stop\\n\")\n",
        "\n",
        "    for i, image in enumerate(images[:10]):  # Process first 10 images\n",
        "        try:\n",
        "            print(f\"\\n--- Processing Vehicle {i+1} ---\")\n",
        "\n",
        "            # Process image\n",
        "            plate_text, confidence, method, license_plate = anpr_system.process_image(image)\n",
        "\n",
        "            if plate_text:\n",
        "                print(f\" License Plate Detected: {plate_text}\")\n",
        "                print(f\"   Confidence: {confidence:.2f}\")\n",
        "                print(f\"   Method: {method}\")\n",
        "\n",
        "                # Display result\n",
        "                plt.figure(figsize=(12, 4))\n",
        "\n",
        "                plt.subplot(1, 2, 1)\n",
        "                img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                plt.imshow(img_rgb)\n",
        "                plt.title('Original Image')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(1, 2, 2)\n",
        "                if license_plate is not None:\n",
        "                    plate_rgb = cv2.cvtColor(license_plate, cv2.COLOR_BGR2RGB)\n",
        "                    plt.imshow(plate_rgb)\n",
        "                    plt.title(f'Detected Plate: {plate_text}')\n",
        "                else:\n",
        "                    plt.text(0.5, 0.5, 'No Plate',\n",
        "                            horizontalalignment='center', verticalalignment='center',\n",
        "                            transform=plt.gca().transAxes, fontsize=14)\n",
        "                    plt.gca().set_facecolor('lightgray')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                print(\" No license plate detected\")\n",
        "\n",
        "            # Simulate delay\n",
        "            import time\n",
        "            time.sleep(delay)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nSimulation stopped by user\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "# Run simulation\n",
        "simulate_real_time_processing(images)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Performance Analysis\n",
        "\n",
        "# %%\n",
        "def analyze_performance(images, labels):\n",
        "    \"\"\"Analyze ANPR system performance\"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(\"Analyzing ANPR system performance...\")\n",
        "\n",
        "    for i, image in enumerate(images[:20]):  # Analyze first 20 images\n",
        "        # Process image\n",
        "        plate_text, confidence, method, _ = anpr_system.process_image(image)\n",
        "\n",
        "        # Get ground truth (simplified - using filename)\n",
        "        ground_truth = labels[i] if i < len(labels) else \"Unknown\"\n",
        "\n",
        "        # Calculate accuracy metrics\n",
        "        if plate_text and ground_truth != \"Unknown\":\n",
        "            # Simple accuracy check (in real scenario, use proper text comparison)\n",
        "            is_correct = plate_text in ground_truth or ground_truth in plate_text\n",
        "            results.append({\n",
        "                'image_id': i,\n",
        "                'detected_text': plate_text,\n",
        "                'ground_truth': ground_truth,\n",
        "                'confidence': confidence,\n",
        "                'method': method,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                'image_id': i,\n",
        "                'detected_text': plate_text,\n",
        "                'ground_truth': ground_truth,\n",
        "                'confidence': confidence,\n",
        "                'method': method,\n",
        "                'is_correct': False\n",
        "            })\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_images = len(results_df)\n",
        "    successful_detections = len(results_df[results_df['detected_text'].notna()])\n",
        "    correct_detections = len(results_df[results_df['is_correct'] == True])\n",
        "\n",
        "    detection_rate = successful_detections / total_images * 100\n",
        "    accuracy_rate = correct_detections / successful_detections * 100 if successful_detections > 0 else 0\n",
        "\n",
        "    print(f\"\\n Performance Summary:\")\n",
        "    print(f\"Total Images Processed: {total_images}\")\n",
        "    print(f\"Successful Detections: {successful_detections}\")\n",
        "    print(f\"Correct Recognitions: {correct_detections}\")\n",
        "    print(f\"Detection Rate: {detection_rate:.1f}%\")\n",
        "    print(f\"Accuracy Rate: {accuracy_rate:.1f}%\")\n",
        "\n",
        "    # Plot performance metrics\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    methods_count = results_df['method'].value_counts()\n",
        "    plt.pie(methods_count.values, labels=methods_count.index, autopct='%1.1f%%')\n",
        "    plt.title('Recognition Methods Used')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    confidence_scores = results_df[results_df['confidence'] > 0]['confidence']\n",
        "    plt.hist(confidence_scores, bins=10, alpha=0.7, edgecolor='black')\n",
        "    plt.title('Confidence Score Distribution')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    success_data = [successful_detections, total_images - successful_detections]\n",
        "    plt.bar(['Successful', 'Failed'], success_data, color=['green', 'red'])\n",
        "    plt.title('Detection Success Rate')\n",
        "    plt.ylabel('Number of Images')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Analyze performance\n",
        "performance_df = analyze_performance(images, labels)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Model Saving and Deployment\n",
        "\n",
        "# %%\n",
        "# Save the trained models and components\n",
        "char_model.save('character_recognition_model.h5')\n",
        "print(\"Character recognition model saved\")\n",
        "\n",
        "# Save plate detector configuration\n",
        "import pickle\n",
        "with open('plate_detector.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'min_area': plate_detector.min_area,\n",
        "        'max_area': plate_detector.max_area\n",
        "    }, f)\n",
        "print(\"Plate detector configuration saved\")\n",
        "\n",
        "# Save character classes\n",
        "with open('character_classes.pkl', 'wb') as f:\n",
        "    pickle.dump(char_classes, f)\n",
        "print(\"Character classes saved\")\n",
        "\n",
        "# Save complete ANPR system configuration\n",
        "anpr_config = {\n",
        "    'input_shape': (32, 32, 1),\n",
        "    'num_classes': len(char_classes),\n",
        "    'char_classes': char_classes,\n",
        "    'model_architecture': 'CNN_LSTM_ANPR'\n",
        "}\n",
        "\n",
        "with open('anpr_config.json', 'w') as f:\n",
        "    import json\n",
        "    json.dump(anpr_config, f, indent=2)\n",
        "print(\"ANPR configuration saved\")\n",
        "\n",
        "# Create deployment report\n",
        "deployment_files = [\n",
        "    'character_recognition_model.h5',\n",
        "    'plate_detector.pkl',\n",
        "    'character_classes.pkl',\n",
        "    'anpr_config.json'\n",
        "]\n",
        "\n",
        "print(f\"\\n Deployment package created with {len(deployment_files)} files\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 12. Advanced Features and Improvements\n",
        "\n",
        "# %%\n",
        "# Additional advanced ANPR features\n",
        "class AdvancedANPRFeatures:\n",
        "    \"\"\"Advanced features for ANPR system\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def enhance_image_quality(image):\n",
        "        \"\"\"Enhance image quality for better recognition\"\"\"\n",
        "        # Convert to LAB color space\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "\n",
        "        # Apply CLAHE to L channel\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "        l_enhanced = clahe.apply(l)\n",
        "\n",
        "        # Merge channels and convert back to BGR\n",
        "        lab_enhanced = cv2.merge([l_enhanced, a, b])\n",
        "        enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_multiple_plates(image):\n",
        "        \"\"\"Detect multiple license plates in single image\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        plates = []\n",
        "\n",
        "        # Use different preprocessing techniques\n",
        "        techniques = [\n",
        "            cv2.Canny(gray, 50, 150),\n",
        "            cv2.Canny(gray, 100, 200),\n",
        "            cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                 cv2.THRESH_BINARY, 11, 2)\n",
        "        ]\n",
        "\n",
        "        for technique in techniques:\n",
        "            contours = cv2.findContours(technique, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            contours = imutils.grab_contours(contours)\n",
        "\n",
        "            for contour in contours:\n",
        "                if cv2.contourArea(contour) > 1000:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    aspect_ratio = w / h\n",
        "\n",
        "                    # Check if contour resembles license plate\n",
        "                    if 2.0 <= aspect_ratio <= 5.0:\n",
        "                        plate = image[y:y+h, x:x+w]\n",
        "                        plates.append((plate, (x, y, w, h)))\n",
        "\n",
        "        return plates\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_license_plate_format(text):\n",
        "        \"\"\"Validate license plate format\"\"\"\n",
        "        # Common license plate patterns\n",
        "        patterns = [\n",
        "            r'^[A-Z]{3}\\d{3}$',      # ABC123\n",
        "            r'^\\d{3}[A-Z]{3}$',      # 123ABC\n",
        "            r'^[A-Z]{2}\\d{4}$',      # AB1234\n",
        "            r'^\\d{4}[A-Z]{2}$',      # 1234AB\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            if re.match(pattern, text):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "# Test advanced features\n",
        "print(\"Testing Advanced ANPR Features...\")\n",
        "advanced_features = AdvancedANPRFeatures()\n",
        "\n",
        "# Test image enhancement\n",
        "if len(images) > 0:\n",
        "    enhanced_image = advanced_features.enhance_image_quality(images[0])\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Enhanced Image')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "#\n",
        "# This notebook demonstrates a complete Automatic Number Plate Recognition (ANPR) system with:\n",
        "#\n",
        "# ### Key Features:\n",
        "# - **Automatic dataset loading** from Kaggle with fallback to sample data\n",
        "# - **License plate detection** using computer vision techniques\n",
        "# - **Character segmentation** for individual character extraction\n",
        "# - **Dual recognition methods** (CNN + OCR) for robust performance\n",
        "# - **Real-time processing simulation**\n",
        "# - **Performance analysis** and metrics\n",
        "# - **Model persistence** for deployment\n",
        "#\n",
        "# ### Technical Components:\n",
        "# - OpenCV for image processing and contour detection\n",
        "# - TensorFlow/Keras for deep learning-based character recognition\n",
        "# - EasyOCR for optical character recognition\n",
        "# - Comprehensive preprocessing pipeline\n",
        "# - Multiple validation techniques\n",
        "#\n",
        "# ### Applications:\n",
        "# - Traffic monitoring and enforcement\n",
        "# - Parking management systems\n",
        "# - Toll collection automation\n",
        "# - Vehicle access control\n",
        "# - Law enforcement applications\n",
        "#\n",
        "# The system can be further improved with:\n",
        "# - Larger and more diverse training datasets\n",
        "# - Advanced deep learning architectures (YOLO, SSD for detection)\n",
        "# - Real-time video processing capabilities\n",
        "# - Multi-country license plate support\n",
        "# - Integration with database systems for vehicle verification"
      ],
      "metadata": {
        "id": "XUcQMzxe4TYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}